[{"authors":null,"categories":null,"content":"I am a third year Phd Candidate of computer science at Michigan State University, SEIT(SEcure and Intelligent Things) lab and eLANS(experimental Laboratory for Advanced Networks and Systems) lab. My research interests include signal processing, speech recognition, machine learning, AI-enabled mobile systems. I am co-advised by Dr. Xiao Li and Dr. Qiben Yan.\n  Download my resum√©.\n","date":1607817600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1607817600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a third year Phd Candidate of computer science at Michigan State University, SEIT(SEcure and Intelligent Things) lab and eLANS(experimental Laboratory for Advanced Networks and Systems) lab. My research interests include signal processing, speech recognition, machine learning, AI-enabled mobile systems.","tags":null,"title":"Hanqing Guo","type":"authors"},{"authors":["Chenning Li","Hanqing Guo","Shuai Tong","Xiao Zeng","Zhichao Cao","Mi Zhang","Qiben Yan","Li Xiao","Jiliang Wang","Yunhao Liu"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641089022,"objectID":"5da359791df8949e5ce4f126921338ae","permalink":"https://example.com/publication/li-2021-nelora/","publishdate":"2022-01-02T02:03:40.111816Z","relpermalink":"/publication/li-2021-nelora/","section":"publication","summary":"","tags":[],"title":"NELoRa: Towards Ultra-low SNR LoRa Communication with Neural-enhanced Demodulation","type":"publication"},{"authors":["Nikolay Ivanov","Hanqing Guo","Qiben Yan"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641089020,"objectID":"18435a9c1ce03329a7b0d8cb6fe24b61","permalink":"https://example.com/publication/ivanov-2021-rectifying/","publishdate":"2022-01-02T02:03:38.259445Z","relpermalink":"/publication/ivanov-2021-rectifying/","section":"publication","summary":"","tags":[],"title":"Rectifying Administrated ERC20 Tokens","type":"publication"},{"authors":["Hanqing Guo","Âê≥ÊÅ©ÈÅî"],"categories":["Demo","ÊïôÁ®ã"],"content":"Overview  The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It\u0026rsquo;s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more    The template is mobile first with a responsive design to ensure that your site looks stunning on every device.  Get Started  üëâ Create a new site üìö Personalize your site üí¨ Chat with the Wowchemy community or Hugo community üê¶ Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy üí° Request a feature or report a bug for Wowchemy ‚¨ÜÔ∏è Updating Wowchemy? View the Update Tutorial and Release Notes  Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n‚ù§Ô∏è Click here to become a sponsor and help support Wowchemy\u0026rsquo;s future ‚ù§Ô∏è As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features ü¶Ñ‚ú®\nEcosystem  Hugo Academic CLI: Automatically import publications from BibTeX  Inspiration Check out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures  Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, ‰∏≠Êñá, and Portugu√™s Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://example.com/post/getting-started/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome üëã We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","ÂºÄÊ∫ê"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":["Hanqing Guo","Nan Zhang","Shaoen Wu","Qing Yang"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641089016,"objectID":"1de030d0be5346c005a9c6a064eb8846","permalink":"https://example.com/publication/guo-2020-deep/","publishdate":"2022-01-02T02:03:34.737101Z","relpermalink":"/publication/guo-2020-deep/","section":"publication","summary":"","tags":[],"title":"Deep Learning Driven Wireless Real-time Human Activity Recognition","type":"publication"},{"authors":["Qiben Yan","Kehai Liu","Qin Zhou","Hanqing Guo","Ning Zhang"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641089014,"objectID":"10e983a2f2f4247c64b6e66c75a86433","permalink":"https://example.com/publication/yan-2020-surfingattack/","publishdate":"2022-01-02T02:03:32.960528Z","relpermalink":"/publication/yan-2020-surfingattack/","section":"publication","summary":"","tags":[],"title":"Surfingattack: Interactive hidden attack on voice assistants using ultrasonic guided waves","type":"publication"},{"authors":null,"categories":null,"content":"source code\rThis link behind shows how to implement YOLOv3 with pytorch instructions\rIn my tutorial, I give a solution to convert Label to Chinese Character.\r4 Steps To Convert Label to Chinese\r1. Change coconame\r First of all, we need to translate english label to chinese label\rtranslated labels is availiable here\n2. Use FreeType to write a script to draw chinese characters\r Because Opencv doesn't support chinese character, we need to use a known font style to draw chinese character, it contains 3 parts:\r\rdraw_ft_bitmap(self, img, bitmap, pen, color)\ndraw each char \r:param bitmap: bitmap\r:param pen: pen\r:param color: pen color e.g.(0,0,255) - red\r:return: image\r\r\rdraw_string(self, img, x_pos, y_pos, text, color)\ndraw string \r:param x_pos: text x-postion on img\r:param y_pos: text y-postion on img\r:param text: text (unicode)\r:param color: text color\r:return: image\r\r\rdraw_text(self, image, pos, text, text_size, text_color)\n(draw chinese(or not) text with ttf \r:param image: image(numpy.ndarray) to draw text\r:param pos: where to draw text\r:param text: the context, for chinese should be unicode type\r:param text_size: text size\r:param text_color:text color\r:return: image\r\r\r\r3. Download msyh.ttf\r .ttf file is dependency to specify font style, it would be loaded in FreeType script, click here to Download 4. Apply ft2 Chinese Character to yolo\rIn main detect flow (cam_detect.py), instead of using cv2.imshow(), we use ft.draw_text()\n\rcomment cv2.putText part, use ft.draw_text instead \r# c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\r# cv2.rectangle(img, c1, c2,color, -1)\r# cv2.rectangle(img, c1, c2, color, -1)\r# cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 2, [225,255,255], 1);\rft = ft2.put_chinese_text('msyh.ttf')\rft.draw_text(image=img, pos=(c1[0], c1[1] + t_size[1] - 7), text=label, text_size=15, text_color=[255, 255, 255])\r\nBefore Translate\r\r\rAfter Translate\r\r\rYou can Download All source code from my github,here\r","date":1560988800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560988800,"objectID":"ab7f12d970eef71622406d0e453a3b32","permalink":"https://example.com/project/yolo/","publishdate":"2019-06-20T00:00:00Z","relpermalink":"/project/yolo/","section":"project","summary":"YOLO V3- dictionary translation! (Â≠óÂÖ∏ËΩ¨Êç¢ÔºÅ)","tags":["Deep Learning"],"title":"YOLO V3 Implementation with Pytorch \u0026 Change Language","type":"project"},{"authors":null,"categories":null,"content":"\r.column {\rfloat: left;\rwidth: 50%;\rpadding: 5px;\r}\r/* Clear floats after image containers */\r.row::after {\rcontent: \"\";\rclear: both;\rdisplay: table;\r}\r\rWalabot senses environment by transmitting, receiving and recording signals from MIMO antennas. The frequency range from 3.3-10 GHz. Today I am going to show you how walabot collect 3D images.\nRaw signals\r\r\rOur radar sensing platform emits probing pulse signals x(t) at a pulse repetition frequency (PRF) of 16 Hz, but within each pulse repetition interval (PRI), the receiver antenna\rsamples the received signal y(t) at a very high frequency of 8 KHz.\rThe x-axis of raw signals is response time, while y-axis means amplitude at that time slot. Red line is actually samples what receiver antenna sampling.\rIn other words, response time is signal traverse time from transmit antenna to receive antenna, which depends on distance to radar. Higher amplitude at specific time, means there is object at that place.\r2D images\r\r\r\rWhile 2D images only shows \u0026phi; (wide angle) versus R;. However, any object in real world is 3D, it has height as well, then introduce \u0026theta; (elevation angle) as height.\rTo get 3D image, let's see the axis system in walabot.\nwhere \rX = R*Sin\u0026theta;\nY = R*Cos\u0026theta;Sin\u0026phi;\nZ = R*Cos\u0026theta;Cos\u0026phi;\r\r3D images\rInstead of using 2D images, we construct 3D images based on those 2D images by stacking them in vertical direction.\nFigure above shows how to stack 2D images, the implement process are\r\rConcatenate 2D images to 3D matrix \rUse measure.marching_cubes_classic to make vertices and faces\rChange axis ranges from interval index to real unit\nFor example: \r(0,100) = (1,200) R(cm)\n(0,61) = (-90,90) \u0026phi;(degree)\n(0,9) = (-20,20) \u0026theta;(degree)\r\r\rUse Poly3DCollection to get mesh\r\r3D videos\rOnce get 3D image, we save it to IO buffer, and use PIL to open buffer, then convert it to ndarray, write ndarray as one frame to video by using openCV\n\rVideo above shows a human walk around walabot radar.\nCNN extract Features\rFor each frame, we use resnet-18 to extract features, we change last Average pooling to Max pooling because Max pooling extracts the most important features like edges whereas,\raverage pooling extracts features so smoothly. For image data, you can see the difference. Although both are used for same reason, I think max pooling is better for extracting the extreme features.\rAverage pooling sometimes can‚Äôt extract good features because it takes all into count and results an average value which may/may not be important for object detection type tasks.\rThen change output linear layer to extract 10 features.\rLSTM training\rWe use 3 frames to recognize one activity. Since 3D radar signal shown above is abstract video. Unlike camera videos which each frame represent a activity, radar video can only be detected by\rcontinuous frames change to recognize one activity. We collect many 3 frames video for each activity as training data. When testing, given continuous stream radar video, and feed every 3 frames to\rLSTM network, then gives our prediction for each frame.\rDetailed technique will be present in my next paper.\r","date":1556323200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556323200,"objectID":"608e5a41a92d0afff1bb3898ac2739e8","permalink":"https://example.com/project/radar/","publishdate":"2019-04-27T00:00:00Z","relpermalink":"/project/radar/","section":"project","summary":"3D radar- visualize your motion with signals!","tags":["Deep Learning"],"title":"Walabot Shows 3D Image","type":"project"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot;\rif porridge == \u0026quot;blueberry\u0026quot;:\rprint(\u0026quot;Eating...\u0026quot;)\r  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}}\r{{% fragment %}} **Two** {{% /fragment %}}\r{{% fragment %}} Three {{% /fragment %}}\r Press Space to play!\nOne \r**Two** \rThree \r A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}}\r- Only the speaker can read these notes\r- Press `S` key to view\r{{% /speaker_note %}}\r Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}}\r{{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}}\r{{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}\r  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1,\r.reveal section h2,\r.reveal section h3 {\rcolor: navy;\r}\r  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://example.com/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Junhong Xu","Shangyue Zhu","Hanqing Guo","Shaoen Wu"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641089018,"objectID":"d4558ad3129d98c40b4872279f7390d3","permalink":"https://example.com/publication/xu-2019-automated/","publishdate":"2022-01-02T02:03:36.462571Z","relpermalink":"/publication/xu-2019-automated/","section":"publication","summary":"","tags":[],"title":"Automated labeling for robotic autonomous navigation through multi-sensory semi-supervised learning on big data","type":"publication"},{"authors":["Hanqing Guo","Shaoen Wu","Honggang Wang","Mahmoud Daneshmand"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641088993,"objectID":"889fca5979c75b8c55030134dc0b4e9d","permalink":"https://example.com/publication/guo-2019-dsic/","publishdate":"2022-01-02T02:03:12.217089Z","relpermalink":"/publication/guo-2019-dsic/","section":"publication","summary":"","tags":[],"title":"DSIC: Deep learning based self-interference cancellation for in-band full duplex wireless","type":"publication"},{"authors":["Shaoen Wu","Hanqing Guo","Junhong Xu","Shangyue Zhu","Honggang Wang"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641089007,"objectID":"04d16828352b93e692937958ec04ac0f","permalink":"https://example.com/publication/wu-2019-band/","publishdate":"2022-01-02T02:03:25.985893Z","relpermalink":"/publication/wu-2019-band/","section":"publication","summary":"","tags":[],"title":"In-band full duplex wireless communications and networking for iot devices: Progress, challenges and opportunities","type":"publication"},{"authors":["Hanqing Guo"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641089005,"objectID":"ce2b5eb134f1de8da51529adb84f7b75","permalink":"https://example.com/publication/guo-2019-real/","publishdate":"2022-01-02T02:03:24.106195Z","relpermalink":"/publication/guo-2019-real/","section":"publication","summary":"","tags":[],"title":"Real-time human activity recognition based on radar","type":"publication"},{"authors":null,"categories":null,"content":"source code\rThis link behind shows how to implement YOLOv3 with pytorch instructions\rIn my tutorial, I give a solution to convert Label to Chinese Character.\r4 Steps To Convert Label to Chinese\r1. Change coconame\r First of all, we need to translate english label to chinese label\rtranslated labels is availiable here\n2. Use FreeType to write a script to draw chinese characters\r Because Opencv doesn't support chinese character, we need to use a known font style to draw chinese character, it contains 3 parts:\r\rdraw_ft_bitmap(self, img, bitmap, pen, color)\ndraw each char \r:param bitmap: bitmap\r:param pen: pen\r:param color: pen color e.g.(0,0,255) - red\r:return: image\r\r\rdraw_string(self, img, x_pos, y_pos, text, color)\ndraw string \r:param x_pos: text x-postion on img\r:param y_pos: text y-postion on img\r:param text: text (unicode)\r:param color: text color\r:return: image\r\r\rdraw_text(self, image, pos, text, text_size, text_color)\n(draw chinese(or not) text with ttf \r:param image: image(numpy.ndarray) to draw text\r:param pos: where to draw text\r:param text: the context, for chinese should be unicode type\r:param text_size: text size\r:param text_color:text color\r:return: image\r\r\r\r3. Download msyh.ttf\r .ttf file is dependency to specify font style, it would be loaded in FreeType script, click here to Download 4. Apply ft2 Chinese Character to yolo\rIn main detect flow (cam_detect.py), instead of using cv2.imshow(), we use ft.draw_text()\n\rcomment cv2.putText part, use ft.draw_text instead \r# c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\r# cv2.rectangle(img, c1, c2,color, -1)\r# cv2.rectangle(img, c1, c2, color, -1)\r# cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 2, [225,255,255], 1);\rft = ft2.put_chinese_text('msyh.ttf')\rft.draw_text(image=img, pos=(c1[0], c1[1] + t_size[1] - 7), text=label, text_size=15, text_color=[255, 255, 255])\r\nBefore Translate\r\r\rAfter Translate\r\r\rYou can Download All source code from my github,here\r","date":1545868800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545868800,"objectID":"1bfbd5d66447c5e03d94e89c41bcff31","permalink":"https://example.com/project/deepfake/","publishdate":"2018-12-27T00:00:00Z","relpermalink":"/project/deepfake/","section":"project","summary":"Deepfake- Swap your face with Trump!","tags":["Deep Learning"],"title":"Deepfakes Implementation with Pytorch","type":"project"},{"authors":["Shaoen Wu","Junhong Xu","Shangyue Zhu","Hanqing Guo"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641089004,"objectID":"43d06d9e02f35381e7fb6207d2ea43c9","permalink":"https://example.com/publication/wu-2018-deep/","publishdate":"2022-01-02T02:03:22.413299Z","relpermalink":"/publication/wu-2018-deep/","section":"publication","summary":"","tags":[],"title":"A deep residual convolutional neural network for facial keypoint detection with missing labels","type":"publication"},{"authors":["Shangyue Zhu","Hanqing Guo","Junhong Xu","Shaoen Wu"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641089000,"objectID":"9f9b07fafbb9944fc0a150625354e04c","permalink":"https://example.com/publication/zhu-2018-distance/","publishdate":"2022-01-02T02:03:18.940714Z","relpermalink":"/publication/zhu-2018-distance/","section":"publication","summary":"","tags":[],"title":"Distance based user localization and tracking with mechanical ultrasonic beamforming","type":"publication"},{"authors":["Shangyue Zhu","Junhong Xu","Hanqing Guo","Qiwei Liu","Shaoen Wu","Honggang Wang"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641089009,"objectID":"a57da31bc2d11253b7c283a2466af017","permalink":"https://example.com/publication/zhu-2018-indoor/","publishdate":"2022-01-02T02:03:27.733344Z","relpermalink":"/publication/zhu-2018-indoor/","section":"publication","summary":"","tags":[],"title":"Indoor human activity recognition based on ambient radar with signal processing and machine learning","type":"publication"},{"authors":["Juhong Xu","Hanqing Guo","Shaoen Wu"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641088997,"objectID":"f459dc3c3cde97decf611cdd40bf5dad","permalink":"https://example.com/publication/xu-2018-indoor/","publishdate":"2022-01-02T02:03:15.502524Z","relpermalink":"/publication/xu-2018-indoor/","section":"publication","summary":"","tags":[],"title":"Indoor multi-sensory self-supervised autonomous mobile robotic navigation","type":"publication"},{"authors":["Qiwei Liu","Hanqing Guo","Junhong Xu","Honggang Wang","Aron Kageza","Saeed AlQarni","Shaoen Wu"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641089011,"objectID":"2a56709d444c18cea274ccdec3e78481","permalink":"https://example.com/publication/liu-2018-non/","publishdate":"2022-01-02T02:03:29.468999Z","relpermalink":"/publication/liu-2018-non/","section":"publication","summary":"","tags":[],"title":"Non-contact non-invasive heart and respiration rates monitoring with MIMO radar sensing","type":"publication"},{"authors":["Hanqing Guo","Nan Zhang","Wenjun Shi","Saeed AlQarni","Shaoen Wu"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641088992,"objectID":"725f02bfc5eea42c16ae8c03f0d11fe0","permalink":"https://example.com/publication/guo-2018-real/","publishdate":"2022-01-02T02:03:10.608244Z","relpermalink":"/publication/guo-2018-real/","section":"publication","summary":"","tags":[],"title":"Real Time 3D Indoor Human Image Capturing Based on FMCW Radar","type":"publication"},{"authors":["Hanqing Guo","Junhong Xu","Shangyue Zhu","Shaoen Wu"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641088995,"objectID":"159859cc197256e3c6045f8695e5ebea","permalink":"https://example.com/publication/guo-2018-realtime/","publishdate":"2022-01-02T02:03:13.881456Z","relpermalink":"/publication/guo-2018-realtime/","section":"publication","summary":"","tags":[],"title":"Realtime software defined self-interference cancellation based on machine learning for in-band full duplex wireless communications","type":"publication"},{"authors":["Junhong Xu","Hanqing Guo","Aron Kageza","Shaoen Wu","Saeed AlQarni"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641089002,"objectID":"349fc9de38c936d1ea895db9b2754d9e","permalink":"https://example.com/publication/xu-2018-removing/","publishdate":"2022-01-02T02:03:20.698517Z","relpermalink":"/publication/xu-2018-removing/","section":"publication","summary":"","tags":[],"title":"Removing Background with Semantic Segmentation Based on Ensemble Learning","type":"publication"},{"authors":["Junhong Xu","Shangyue Zhu","Hanqing Guo","Shaoen Wu"],"categories":[],"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641088998,"objectID":"0440159222351e3ca5936beb31d595bf","permalink":"https://example.com/publication/xu-2017-avoidance/","publishdate":"2022-01-02T02:03:17.118066Z","relpermalink":"/publication/xu-2017-avoidance/","section":"publication","summary":"","tags":[],"title":"Avoidance of manual labeling in robotic autonomous navigation through multi-sensory semi-supervised learning","type":"publication"},{"authors":["Shaoen Wu","kelly Blair","Junhong Xu","Shangyue Zhu","Hanqing Guo","Kai Wang","Lei Chen"],"categories":[],"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641089012,"objectID":"577d2b8788dc1ef245685617d1bbe8bd","permalink":"https://example.com/publication/wu-2017-real/","publishdate":"2022-01-02T02:03:31.236048Z","relpermalink":"/publication/wu-2017-real/","section":"publication","summary":"","tags":[],"title":"Real Time Video Stitching by Exploring Temporal and Spatial Features","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://example.com/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ff6a19061a984819d30c916886db56ef","permalink":"https://example.com/publication/example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/example/","section":"publication","summary":"","tags":null,"title":"","type":"publication"},{"authors":[],"categories":null,"content":"Motivation:   Query-Inefficient: Existing black box audio attacks require plenty of queries to interact with the victim speech models, e.g., Devil\u0026rsquo;s whisper attacks commercial speech to text model by training an substitute model through querying victim model. OCCAM estimates the gradient by incorporating evolutionary algorithms. The extensive query is time consuming, costly and may attract attention.\n  Decision-Boundary: We find that the decision boundary in speech to text model is different from it in computer vision models. Due to the non-contiguous decision boundary, it\u0026rsquo;s hard for attacker to initialize the perturbation.\n  Phoneme We find a short phoneme can surprisingly alter the speech model prediction. Based on the observation, we decide to optimize the duration and power of phoneme to inject it attacking the speech to text model.\n  Methodology    There are three stages to generate a perturbation. At stage 1, given an input audio, we inject many different phonemes to it and try to alter the transcript result. Next, we collect the phonemes or phoneme combinations to a perturbation set. At stage 3, we estimate the gradient by applying small changes to the candidate perturbation, and then estimate the gradient direction through observing the prediction result changes. Then apply the estimated gradient to fine tuning the perturbation. Finally, we can get the adversarial perturbation targeting to a input audio.\n\r\rBob and Alice talk simultaneously\rOnly Alice's voice is kept\r\r\rYour browser does not support the audio element.\r\rYour browser does not support the audio element.\r\r\r\r --\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"28004624a091121eadc5eba2d3c51836","permalink":"https://example.com/talk/black-box-query-efficient-audio-adversarial-attacks/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/talk/black-box-query-efficient-audio-adversarial-attacks/","section":"event","summary":"Our audio adversarial attacks is a query efficient black-box attack towards voice assistants. Existing black-box adversarial attacks on voice assistants either apply substitution models or leverage the intermediate model output to estimate the gradients for crafting adversarial audio samples. However, these attack approaches require a significant amount of queries with a lengthy training stage. Our attack leverages the decision-based attack to produce effective adversarial audios, and reduces the number of queries by optimizing the gradient estimation. In the experiments, we perform our attack against 4 different speech-to-text APIs under 3 real-world scenarios to demonstrate the real-time attack impact. The results also show that our attack is practical and robust in attacking 5 popular commercial voice controllable devices over the air. The benchmark result shows we can generate adversarial examples and launch the attack in a couple of minutes. We improve the query efficiency and reduce the cost of a successful untargeted and targeted adversarial attack by 93.1% and 65.5% compared with the state-of-the-art black-box attacks, using only‚àº300 and‚àº1,500 queries, respectively.","tags":[],"title":"Black-Box, Query-Efficient Audio Adversarial Attacks","type":"event"},{"authors":[],"categories":null,"content":"Motivation:   Necessity of Recorder Jammer: Voice recording is an essential information-sharing approach, which is benefiting many aspects of our daily life. Nowadays, smartphones and Internet-of-Things (IoT) devices equipped with microphones allow people to record voice anytime and anywhere. However, the growing presence of unauthorized microphones has led to numerous incidences of privacy violations. Off-the-shelf microphones are widely available and can be deployed to steal users' biometric traits (e.g. voiceprints) or private conversations. Thus, unauthorized voice recording has become a serious societal issue.\n  Limitation of Existing Jammer: Existing jammers disrupt unauthorized voice recording by emiting an ultrasounic scrambling noise. However, there are two limitations of those applications/devices.\n It will affact the normal usage of other voice users. For example, in the cover figure, when Bob deploys such jammer in public, all the microphones will be jammed, which means the surrounded people cannnot use their voice assistants, video call, and enmergency call meanwhile. It\u0026rsquo;s not secure enough. The jammed recording can be parsed if the attacker know the noise pattern. If the attacker learns the frequency pattern of the scrambling noise wave, the attacker can deploy an additional microphone to nullify the noises and record them illegally    Speaker specified feature: We found every speaker has a distinct speech feature that introduced by the variety of their vocal system. By adopting the speaker specified feature, we can extract Bob\u0026rsquo;s voice from a mixed audio input. Then we use a ultrasound speaker to play the rest sound (e.g., Alice\u0026rsquo;s voice) that not belong to Bob, and overshadow Bob\u0026rsquo;s voice in the wild while maintaining the other speakers' voice on their end.     Methodology    The core of our model is the speaker Selector that present in above figure. We use the Selector to extract Bob\u0026rsquo;s voice in real-time. To train the Selector, we construct many mixed audio from two speakers, and introduce the prior knowledge (Reference Audio) of the one speaker in the mixed audio. Through filtering out the known speaker from the mixed audio, the Selector learns to extract speaker specified voice from the mixed sound. Then, the Selector produces the filtered Bob\u0026rsquo;s irrelated to mix the original sound, and hence hide Bob\u0026rsquo;s voice on the recorder side.\nDemo    Setup-1 Setup-2              There are two experimental settings of our work. Left is the benchmark test running with public dataset and simulate the mixed speaker plays the mixed audio. Right is the real world scenario that Bob holds our device to protect his voice without intervening Alice\u0026rsquo;s phone use.\nBelow are the voice demos of our system that recorded from Alice\u0026rsquo;s recorder.\n\r\r\rBob and Alice talk simultaneously\rOnly Alice's voice is kept\r\r\rYour browser does not support the audio element.\r\rYour browser does not support the audio element.\r\r\r\rYour browser does not support the audio element.\r\rYour browser does not support the audio element.\r\r\r\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"30eba5c397f1e9c7b33ec6d66ef37236","permalink":"https://example.com/talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/","section":"event","summary":"In this paper, we propose NEC (Neural Enhanced Cancellation), a defense mechanism, which prevents unauthorized microphones from capturing a target speaker's voice. Compared with the existing scrambling-based audio cancellation approaches, NEC can selectively remove a target speaker's voice from a mixed speech without causing interference to others. Specifically, for a target speaker, we design a Deep Neural Network (DNN) model to extract high-level speaker-specific but utterance-independent vocal features from his/her reference audios. During the microphone recording, the DNN generates a shadow sound to cancel the target voice in real-time. Moreover, we modulate the audible shadow sound onto an ultrasound frequency, making it inaudible for humans. By leveraging the non-linearity of the microphone circuit, the microphone can accurately decode the shadow sound for target voice cancellation. We implement and evaluate NEC comprehensively with 8 smartphone microphones in different settings. The results show that NEC effectively mutes the target speaker at a microphone without interfering with other users' normal conversations.","tags":[],"title":"Speaker Selective Cancellation via Neural Enhanced Ultrasound Shadowing","type":"event"},{"authors":[],"categories":null,"content":"Motivation:   Ultrasound in Speech: We found that human can produce ultrasound by collecting human speech with a high-end ultrasonic microphone (CM-16). See the experimental setup below:   From the recorded human speech, we analyze the spectrogram and present it to the next figure.   It shows that when participants say \u0026ldquo;She had your dark suit in greasy wash water all year\u0026rdquo;, there are some phonemes such as /sh/, /s/, /sy/, /s/ reach extremely high frequencies (e.g., ~40kHz). The reason why human can produce some ultrasound is the different manners of articulation make different airstream flows. When two speech organs narrow the airstream to cause friction to occur as it passes through, Fricatives are produced. If the airstream is stopped and then released, Stop or Affricate is produced. We found that fricative, stop, and affricate phonemes often carries more energy in ultrasound band.\n  Distinctiveness of Ultrasound: We extract the speech energies at fricative, stop, and affricate phonemes and summerize them together to represent the identity of the speakers by using $$S_{LTA}(f)=\\frac{1}{M}\\sum_{t=1}^{N} S(f, p)$$ where S(f,p) represents the spectrogram value at frequency f and time frame p. By comparing the $$S_{LTA}(f)$$ of different participants, we get the following results.   From the left figure, we can see that participants have dispersed speech energies between frequency range from 20kHz-40kHz, for lower frequencies, we observed similar speech energy trends. The right figure shows the average variance of those speakers, it is evident to show that 16-24kHz, 24-32kHz and 32-40kHz contribute more differences to differentiate participants.\n  Ultrasound Speech for Liveness Detection Since we find that human can produce some ultrasound, we then use them to detect whether a sound is generated by human or a speaker. Through recording the human speech and the replay sound, we present the spectrogram differences as follow:    Human: Human speech directly recorded by an ultrasound microphone. Scenario1: Attackers record and replay with commercial devices (smart phones). Scenario2: Attackers record with high-end microphones and replay with commercial speakers (smart phones). Scenario3: Attackers record with high-end microphones and replay with ultrasound speakers.    We can observe the apparent difference for four spectrograms. By exploiting this clear differences, we can design our liveness detection algorithm to reach high detection accuracy.\nMethodology    We design a end to end speaker authentication system, which includes a liveness detection module and a speaker verification module. By incorporating the ultrasound component in human speech, we design a Cumulative energy analysis method to detect replay attack. Furthermore, we introduce a two-stream DNN to handle the low frequency data and the high frequency speech, and fused the extracted feature together to verify the user\u0026rsquo;s voice.\nDemo For more descripitions and demos of our collected high frequency human speech, please visit Here.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"95aea0d1b3d1e4ed0b2e2a83e9c1a515","permalink":"https://example.com/talk/ultrasound-enabled-speaker-authentication/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/talk/ultrasound-enabled-speaker-authentication/","section":"event","summary":"Existing speaker verification techniques distinguish individual speakers via the spectrographic features extracted from an audible frequency range of voice commands. However, they often have high error rates and/or long delays. In this paper, we explore a new direction of human voice research by scrutinizing the unique characteristics of human speech at the ultrasound frequency band. Our research indicates that the high-frequency ultrasound components (e.g. speech fricatives) from 20 to 48 kHz can significantly enhance the security and accuracy of speaker verification. We propose a speaker verification system, which uses a two-stream DNN architecture with a feature fusion mechanism to generate distinctive speaker models. To test the system, we create a speech dataset with 12 hours of audio (9,050 voice samples) from 128 participants. In addition, we create a second spoofed voice dataset to evaluate its security. In order to balance between controlled recordings and real-world applications, the audio recordings are collected from two quiet rooms by 8 different recording devices, including 7 smartphones and an ultrasound microphone. Our evaluation shows that our system achieves 0.58% equal error rate in the speaker verification task, which reduces the best equal error rate of the existing systems by 86.1%. Besides, our system only takes 120 ms for testing an incoming utterance,  within 91 ms processing time, we achieve 0% equal error rate in detecting replay attacks launched by 5 different loudspeakers.","tags":[],"title":"Ultrasound Enabled Speaker Authentication","type":"event"}]