<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Recent &amp; Upcoming Talks | Academic</title>
    <link>https://example.com/event/</link>
      <atom:link href="https://example.com/event/index.xml" rel="self" type="application/rss+xml" />
    <description>Recent &amp; Upcoming Talks</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 01 Jan 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://example.com/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Recent &amp; Upcoming Talks</title>
      <link>https://example.com/event/</link>
    </image>
    
    <item>
      <title>Black-Box, Query-Efficient Audio Adversarial Attacks</title>
      <link>https://example.com/talk/black-box-query-efficient-audio-adversarial-attacks/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://example.com/talk/black-box-query-efficient-audio-adversarial-attacks/</guid>
      <description>&lt;h3 id=&#34;motivation&#34;&gt;&lt;strong&gt;Motivation&lt;/strong&gt;:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Query-Inefficient&lt;/strong&gt;: Existing black box audio attacks require plenty of queries to interact with the victim speech models, e.g., &lt;a href=&#34;https://www.usenix.org/system/files/sec20summer_chen-yuxuan_prepub.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Devil&amp;rsquo;s whisper&lt;/a&gt; attacks commercial speech to text model by training an substitute model through querying victim model. &lt;a href=&#34;https://arxiv.org/pdf/2110.09714.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OCCAM&lt;/a&gt; estimates the gradient by incorporating evolutionary algorithms. The extensive query is time consuming, costly and may attract attention.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Decision-Boundary&lt;/strong&gt;: We find that the decision boundary in speech to text model is different from it in computer vision models. Due to the non-contiguous decison boundary, it&amp;rsquo;s hard for attacker to initialize the perturbation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Phoneme&lt;/strong&gt; We find a short phoneme can suprisingly alter the speech model prediction. Based on the observation, we decide to optimize the duration and power of phoneme to inject it attcking the speech to text model.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;methodology&#34;&gt;&lt;strong&gt;Methodology&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;method&#34; srcset=&#34;
               /talk/black-box-query-efficient-audio-adversarial-attacks/method_hu1fb2b889d904d6d5c2d2b18568b844bf_65740_09e22208c629ca2bc018ec837b2af644.JPG 400w,
               /talk/black-box-query-efficient-audio-adversarial-attacks/method_hu1fb2b889d904d6d5c2d2b18568b844bf_65740_c2c486aed395a6a05cadf6cffa5ca2b3.JPG 760w,
               /talk/black-box-query-efficient-audio-adversarial-attacks/method_hu1fb2b889d904d6d5c2d2b18568b844bf_65740_1200x1200_fit_q75_lanczos.JPG 1200w&#34;
               src=&#34;https://example.com/talk/black-box-query-efficient-audio-adversarial-attacks/method_hu1fb2b889d904d6d5c2d2b18568b844bf_65740_09e22208c629ca2bc018ec837b2af644.JPG&#34;
               width=&#34;760&#34;
               height=&#34;250&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
 There are three stages to generate a perturbation. At stage 1, given an input audio, we inject many different phonemes to it and try to alter the transcript result. Next, we collect the phonemes or phoneme combinations to a perturbation set. At stage 3, we estimate the gradient by applying small changes to the candidate perturbation, and then estimate the gradient direction through observing the prediction result changes. Then apply the estimated gradient to fine tuning the perturbation. Finally, we can get the adversarial perturbation targeting to a input audio.&lt;/p&gt;
&lt;h3 id=&#34;demo&#34;&gt;&lt;strong&gt;Demo&lt;/strong&gt;&lt;/h3&gt;
&lt;div&gt;
&lt;table&gt;
&lt;tr&gt;
	&lt;td&gt;Bob and Alice talk simultanously&lt;/td&gt;
	&lt;td&gt;Only Alice&#39;s voice is kept&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
	&lt;td&gt;&lt;audio controls&gt;
		  &lt;source src=&#34;./assets/joint.wav&#34; type=&#34;audio/wav&#34;&gt;
		  Your browser does not support the &lt;code&gt;audio&lt;/code&gt; element.
		&lt;/audio&gt;&lt;/td&gt;
	&lt;td&gt;&lt;audio controls&gt;
		  &lt;source src=&#34;./assets/joint-focus.wav&#34; type=&#34;audio/wav&#34;&gt;
		  Your browser does not support the &lt;code&gt;audio&lt;/code&gt; element.
		&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Ultrasound Enabled Speaker Authentication</title>
      <link>https://example.com/talk/ultrasound-enabled-speaker-authentication/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://example.com/talk/ultrasound-enabled-speaker-authentication/</guid>
      <description>&lt;h3 id=&#34;motivation&#34;&gt;&lt;strong&gt;Motivation&lt;/strong&gt;:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ultrasound in Speech&lt;/strong&gt;: 















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;method&#34; srcset=&#34;
               /talk/ultrasound-enabled-speaker-authentication/ultrasound_audio_hu1fb2b889d904d6d5c2d2b18568b844bf_44143_bed9d5ea7508815bc6c3e95dede844ff.JPG 400w,
               /talk/ultrasound-enabled-speaker-authentication/ultrasound_audio_hu1fb2b889d904d6d5c2d2b18568b844bf_44143_750aafde5cd2e7168ccbb0118d2239a3.JPG 760w,
               /talk/ultrasound-enabled-speaker-authentication/ultrasound_audio_hu1fb2b889d904d6d5c2d2b18568b844bf_44143_1200x1200_fit_q75_lanczos.JPG 1200w&#34;
               src=&#34;https://example.com/talk/ultrasound-enabled-speaker-authentication/ultrasound_audio_hu1fb2b889d904d6d5c2d2b18568b844bf_44143_bed9d5ea7508815bc6c3e95dede844ff.JPG&#34;
               width=&#34;611&#34;
               height=&#34;255&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
 















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;vocal&#34; srcset=&#34;
               /talk/ultrasound-enabled-speaker-authentication/vocal_hu1fb2b889d904d6d5c2d2b18568b844bf_25846_a1405f5229999b1e073356c99ceefb1c.JPG 400w,
               /talk/ultrasound-enabled-speaker-authentication/vocal_hu1fb2b889d904d6d5c2d2b18568b844bf_25846_d1a8f85ed994d1330a79b09a480d7706.JPG 760w,
               /talk/ultrasound-enabled-speaker-authentication/vocal_hu1fb2b889d904d6d5c2d2b18568b844bf_25846_1200x1200_fit_q75_lanczos.JPG 1200w&#34;
               src=&#34;https://example.com/talk/ultrasound-enabled-speaker-authentication/vocal_hu1fb2b889d904d6d5c2d2b18568b844bf_25846_a1405f5229999b1e073356c99ceefb1c.JPG&#34;
               width=&#34;396&#34;
               height=&#34;360&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Decision-Boundary&lt;/strong&gt;: We find that the decision boundary in speech to text model is different from it in computer vision models. Due to the non-contiguous decison boundary, it&amp;rsquo;s hard for attacker to initialize the perturbation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Phoneme&lt;/strong&gt; We find a short phoneme can suprisingly alter the speech model prediction. Based on the observation, we decide to optimize the duration and power of phoneme to inject it attcking the speech to text model.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;methodology&#34;&gt;&lt;strong&gt;Methodology&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;method.JPG&#34; alt=&#34;method&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
 There are three stages to generate a perturbation. At stage 1, given an input audio, we inject many different phonemes to it and try to alter the transcript result. Next, we collect the phonemes or phoneme combinations to a perturbation set. At stage 3, we estimate the gradient by applying small changes to the candidate perturbation, and then estimate the gradient direction through observing the prediction result changes. Then apply the estimated gradient to fine tuning the perturbation. Finally, we can get the adversarial perturbation targeting to a input audio.&lt;/p&gt;
&lt;h3 id=&#34;demo&#34;&gt;&lt;strong&gt;Demo&lt;/strong&gt;&lt;/h3&gt;
&lt;div&gt;
&lt;table&gt;
&lt;tr&gt;
	&lt;td&gt;Bob and Alice talk simultanously&lt;/td&gt;
	&lt;td&gt;Only Alice&#39;s voice is kept&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
	&lt;td&gt;&lt;audio controls&gt;
		  &lt;source src=&#34;./assets/joint.wav&#34; type=&#34;audio/wav&#34;&gt;
		  Your browser does not support the &lt;code&gt;audio&lt;/code&gt; element.
		&lt;/audio&gt;&lt;/td&gt;
	&lt;td&gt;&lt;audio controls&gt;
		  &lt;source src=&#34;./assets/joint-focus.wav&#34; type=&#34;audio/wav&#34;&gt;
		  Your browser does not support the &lt;code&gt;audio&lt;/code&gt; element.
		&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
