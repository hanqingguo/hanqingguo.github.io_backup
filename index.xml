<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Academic</title>
    <link>https://example.com/</link>
      <atom:link href="https://example.com/index.xml" rel="self" type="application/rss+xml" />
    <description>Academic</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 01 Jan 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://example.com/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Academic</title>
      <link>https://example.com/</link>
    </image>
    
    <item>
      <title>Black-Box, Query-Efficient Audio Adversarial Attacks</title>
      <link>https://example.com/talk/black-box-query-efficient-audio-adversarial-attacks/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://example.com/talk/black-box-query-efficient-audio-adversarial-attacks/</guid>
      <description>&lt;h3 id=&#34;motivation&#34;&gt;&lt;strong&gt;Motivation&lt;/strong&gt;:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Query-Inefficient&lt;/strong&gt;: Existing black box audio attacks require plenty of queries to interact with the victim speech models, e.g., &lt;a href=&#34;https://www.usenix.org/system/files/sec20summer_chen-yuxuan_prepub.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Devil&amp;rsquo;s whisper&lt;/a&gt; attacks commercial speech to text model by training an substitute model through querying victim model. &lt;a href=&#34;https://arxiv.org/pdf/2110.09714.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OCCAM&lt;/a&gt; estimates the gradient by incorporating evolutionary algorithms. The extensive query is time consuming, costly and may attract attention.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Decision-Boundary&lt;/strong&gt;: We find that the decision boundary in speech to text model is different from it in computer vision models. Due to the non-contiguous decision boundary, it&amp;rsquo;s hard for attacker to initialize the perturbation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Phoneme&lt;/strong&gt; We find a short phoneme can surprisingly alter the speech model prediction. Based on the observation, we decide to optimize the duration and power of phoneme to inject it attacking the speech to text model.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;methodology&#34;&gt;&lt;strong&gt;Methodology&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;method&#34; srcset=&#34;
               /talk/black-box-query-efficient-audio-adversarial-attacks/method_hu1fb2b889d904d6d5c2d2b18568b844bf_65740_09e22208c629ca2bc018ec837b2af644.JPG 400w,
               /talk/black-box-query-efficient-audio-adversarial-attacks/method_hu1fb2b889d904d6d5c2d2b18568b844bf_65740_c2c486aed395a6a05cadf6cffa5ca2b3.JPG 760w,
               /talk/black-box-query-efficient-audio-adversarial-attacks/method_hu1fb2b889d904d6d5c2d2b18568b844bf_65740_1200x1200_fit_q75_lanczos.JPG 1200w&#34;
               src=&#34;https://example.com/talk/black-box-query-efficient-audio-adversarial-attacks/method_hu1fb2b889d904d6d5c2d2b18568b844bf_65740_09e22208c629ca2bc018ec837b2af644.JPG&#34;
               width=&#34;760&#34;
               height=&#34;250&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
 There are three stages to generate a perturbation. At stage 1, given an input audio, we inject many different phonemes to it and try to alter the transcript result. Next, we collect the phonemes or phoneme combinations to a perturbation set. At stage 3, we estimate the gradient by applying small changes to the candidate perturbation, and then estimate the gradient direction through observing the prediction result changes. Then apply the estimated gradient to fine tuning the perturbation. Finally, we can get the adversarial perturbation targeting to a input audio.&lt;/p&gt;
&lt;h3 id=&#34;demo&#34;&gt;&lt;strong&gt;Demo&lt;/strong&gt;&lt;/h3&gt;
&lt;div&gt;
&lt;table&gt;
&lt;tr&gt;
	&lt;td&gt;Bob and Alice talk simultaneously&lt;/td&gt;
	&lt;td&gt;Only Alice&#39;s voice is kept&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
	&lt;td&gt;&lt;audio controls&gt;
		  &lt;source src=&#34;./assets/joint.wav&#34; type=&#34;audio/wav&#34;&gt;
		  Your browser does not support the &lt;code&gt;audio&lt;/code&gt; element.
		&lt;/audio&gt;&lt;/td&gt;
	&lt;td&gt;&lt;audio controls&gt;
		  &lt;source src=&#34;./assets/joint-focus.wav&#34; type=&#34;audio/wav&#34;&gt;
		  Your browser does not support the &lt;code&gt;audio&lt;/code&gt; element.
		&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>NELoRa: Towards Ultra-low SNR LoRa Communication with Neural-enhanced Demodulation</title>
      <link>https://example.com/publication/li-2021-nelora/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/li-2021-nelora/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Rectifying Administrated ERC20 Tokens</title>
      <link>https://example.com/publication/ivanov-2021-rectifying/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/ivanov-2021-rectifying/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Speaker Selective Cancellation via Neural Enhanced Ultrasound Shadowing</title>
      <link>https://example.com/talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://example.com/talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/</guid>
      <description>&lt;h3 id=&#34;motivation&#34;&gt;&lt;strong&gt;Motivation&lt;/strong&gt;:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Necessity of Recorder Jammer:&lt;/strong&gt; Voice recording is an essential information-sharing approach, which is benefiting many aspects of our daily life. Nowadays, smartphones and Internet-of-Things (IoT) devices equipped with microphones allow people to record voice anytime and anywhere.
However, the growing presence of unauthorized microphones has led to numerous incidences of privacy violations. Off-the-shelf microphones are widely available and can be deployed to steal users&#39; biometric traits (e.g. voiceprints) or private conversations. Thus, unauthorized voice recording has become a serious societal issue.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Limitation of Existing Jammer:&lt;/strong&gt; Existing jammers disrupt unauthorized voice recording by emiting an ultrasounic scrambling noise. However, there are two limitations of those applications/devices.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It will &lt;strong&gt;affact&lt;/strong&gt; the normal usage of other voice users. For example, in the cover figure, when Bob deploys such jammer in public, all the microphones will be jammed, which means the surrounded people cannnot use their voice assistants, video call, and enmergency call meanwhile.&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s &lt;strong&gt;not secure enough&lt;/strong&gt;. The jammed recording can be parsed if the attacker know the noise pattern. If the attacker learns the frequency pattern of the scrambling noise wave, the attacker can deploy an additional microphone to nullify the noises and record them illegally&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Speaker specified feature&lt;/strong&gt;: We found every speaker has a distinct speech feature that introduced by the variety of their vocal system. By adopting the speaker specified feature, we can extract Bob&amp;rsquo;s voice from a mixed audio input. Then we use a ultrasound speaker to play the rest sound (e.g., Alice&amp;rsquo;s voice) that not belong to Bob, and overshadow Bob&amp;rsquo;s voice in the wild while maintaining the other speakers&#39; voice on their end.
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;demo&#34; srcset=&#34;
               /talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/demo_hu1fb2b889d904d6d5c2d2b18568b844bf_39706_27d441b62bd99f6b05ccf1d5ad59f13b.JPG 400w,
               /talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/demo_hu1fb2b889d904d6d5c2d2b18568b844bf_39706_f124db61864ebc361caff305f8256440.JPG 760w,
               /talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/demo_hu1fb2b889d904d6d5c2d2b18568b844bf_39706_1200x1200_fit_q75_lanczos.JPG 1200w&#34;
               src=&#34;https://example.com/talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/demo_hu1fb2b889d904d6d5c2d2b18568b844bf_39706_27d441b62bd99f6b05ccf1d5ad59f13b.JPG&#34;
               width=&#34;603&#34;
               height=&#34;346&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;methodology&#34;&gt;&lt;strong&gt;Methodology&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/model1_hu1fb2b889d904d6d5c2d2b18568b844bf_50310_ff684e1ceeec991546c5270752b587ef.JPG 400w,
               /talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/model1_hu1fb2b889d904d6d5c2d2b18568b844bf_50310_e3e4ff91bc47c51a524610e7db1627c2.JPG 760w,
               /talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/model1_hu1fb2b889d904d6d5c2d2b18568b844bf_50310_1200x1200_fit_q75_lanczos.JPG 1200w&#34;
               src=&#34;https://example.com/talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/model1_hu1fb2b889d904d6d5c2d2b18568b844bf_50310_ff684e1ceeec991546c5270752b587ef.JPG&#34;
               width=&#34;666&#34;
               height=&#34;268&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;br&gt;
The core of our model is the speaker &lt;strong&gt;Selector&lt;/strong&gt; that present in above figure. We use the &lt;strong&gt;Selector&lt;/strong&gt; to extract Bob&amp;rsquo;s voice in real-time. To train the &lt;strong&gt;Selector&lt;/strong&gt;, we construct many mixed audio from two speakers, and introduce the prior knowledge (&lt;strong&gt;Reference Audio&lt;/strong&gt;) of the one speaker in the &lt;strong&gt;mixed audio&lt;/strong&gt;. Through filtering out the known speaker from the mixed audio, the &lt;strong&gt;Selector&lt;/strong&gt; learns to extract speaker specified voice from the mixed sound. Then, the &lt;strong&gt;Selector&lt;/strong&gt; produces the filtered &lt;strong&gt;Bob&amp;rsquo;s irrelated&lt;/strong&gt; to mix the original sound, and hence hide &lt;strong&gt;Bob&amp;rsquo;s voice&lt;/strong&gt; on the recorder side.&lt;/p&gt;
&lt;h3 id=&#34;demo&#34;&gt;&lt;strong&gt;Demo&lt;/strong&gt;&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Setup-1&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Setup-2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/setup_hu1fb2b889d904d6d5c2d2b18568b844bf_51378_c7e15a61a883c5603ca9d6f540faf8eb.JPG 400w,
               /talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/setup_hu1fb2b889d904d6d5c2d2b18568b844bf_51378_39524779f4b62b91a5761ab8b9f62aa1.JPG 760w,
               /talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/setup_hu1fb2b889d904d6d5c2d2b18568b844bf_51378_1200x1200_fit_q75_lanczos.JPG 1200w&#34;
               src=&#34;https://example.com/talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/setup_hu1fb2b889d904d6d5c2d2b18568b844bf_51378_c7e15a61a883c5603ca9d6f540faf8eb.JPG&#34;
               width=&#34;483&#34;
               height=&#34;347&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/setup2_hu1fb2b889d904d6d5c2d2b18568b844bf_36073_1c526fbced878306993efd729fb86483.JPG 400w,
               /talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/setup2_hu1fb2b889d904d6d5c2d2b18568b844bf_36073_9c49360bdf827aacbaf02e4544fd1231.JPG 760w,
               /talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/setup2_hu1fb2b889d904d6d5c2d2b18568b844bf_36073_1200x1200_fit_q75_lanczos.JPG 1200w&#34;
               src=&#34;https://example.com/talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/setup2_hu1fb2b889d904d6d5c2d2b18568b844bf_36073_1c526fbced878306993efd729fb86483.JPG&#34;
               width=&#34;443&#34;
               height=&#34;376&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;There are two experimental settings of our work. &lt;strong&gt;Left&lt;/strong&gt; is the benchmark test running with public dataset and simulate the mixed speaker plays the mixed audio. &lt;strong&gt;Right&lt;/strong&gt; is the real world scenario that Bob holds our device to protect his voice without intervening Alice&amp;rsquo;s phone use.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Below are the voice demos of our system that recorded from &lt;strong&gt;Alice&amp;rsquo;s recorder&lt;/strong&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;div&gt;
&lt;table&gt;
&lt;tr&gt;
	&lt;td&gt;Bob and Alice talk simultaneously&lt;/td&gt;
	&lt;td&gt;Only Alice&#39;s voice is kept&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
	&lt;td&gt;&lt;audio controls&gt;
		  &lt;source src=&#34;joint.wav&#34; type=&#34;audio/wav&#34;&gt;
		  Your browser does not support the &lt;code&gt;audio&lt;/code&gt; element.
		&lt;/audio&gt;&lt;/td&gt;
	&lt;td&gt;&lt;audio controls&gt;
		  &lt;source src=&#34;joint-focus.wav&#34; type=&#34;audio/wav&#34;&gt;
		  Your browser does not support the &lt;code&gt;audio&lt;/code&gt; element.
		&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
	&lt;td&gt;&lt;audio controls&gt;
		  &lt;source src=&#34;conversation.wav&#34; type=&#34;audio/wav&#34;&gt;
		  Your browser does not support the &lt;code&gt;audio&lt;/code&gt; element.
		&lt;/audio&gt;&lt;/td&gt;
	&lt;td&gt;&lt;audio controls&gt;
		  &lt;source src=&#34;conversation-hide.wav&#34; type=&#34;audio/wav&#34;&gt;
		  Your browser does not support the &lt;code&gt;audio&lt;/code&gt; element.
		&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Ultrasound Enabled Speaker Authentication</title>
      <link>https://example.com/talk/ultrasound-enabled-speaker-authentication/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://example.com/talk/ultrasound-enabled-speaker-authentication/</guid>
      <description>&lt;h3 id=&#34;motivation&#34;&gt;&lt;strong&gt;Motivation&lt;/strong&gt;:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ultrasound in Speech&lt;/strong&gt;: We found that human can produce ultrasound by collecting human speech with a high-end ultrasonic microphone (&lt;a href=&#34;http://www.avisoft.com/ultrasound-microphones/cm16-cmpa/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CM-16&lt;/a&gt;). See the experimental setup below:
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;setup&#34; srcset=&#34;
               /talk/ultrasound-enabled-speaker-authentication/testbed_hu1fb2b889d904d6d5c2d2b18568b844bf_60110_7651c9548bc1480dd37f1c5edab33133.JPG 400w,
               /talk/ultrasound-enabled-speaker-authentication/testbed_hu1fb2b889d904d6d5c2d2b18568b844bf_60110_8257df6de9f21ab2496f1da5da7bf73e.JPG 760w,
               /talk/ultrasound-enabled-speaker-authentication/testbed_hu1fb2b889d904d6d5c2d2b18568b844bf_60110_1200x1200_fit_q75_lanczos.JPG 1200w&#34;
               src=&#34;https://example.com/talk/ultrasound-enabled-speaker-authentication/testbed_hu1fb2b889d904d6d5c2d2b18568b844bf_60110_7651c9548bc1480dd37f1c5edab33133.JPG&#34;
               width=&#34;634&#34;
               height=&#34;385&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
 From the recorded human speech, we analyze the spectrogram and present it to the next figure.
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;method&#34; srcset=&#34;
               /talk/ultrasound-enabled-speaker-authentication/ultrasound_audio_hu1fb2b889d904d6d5c2d2b18568b844bf_44143_bed9d5ea7508815bc6c3e95dede844ff.JPG 400w,
               /talk/ultrasound-enabled-speaker-authentication/ultrasound_audio_hu1fb2b889d904d6d5c2d2b18568b844bf_44143_750aafde5cd2e7168ccbb0118d2239a3.JPG 760w,
               /talk/ultrasound-enabled-speaker-authentication/ultrasound_audio_hu1fb2b889d904d6d5c2d2b18568b844bf_44143_1200x1200_fit_q75_lanczos.JPG 1200w&#34;
               src=&#34;https://example.com/talk/ultrasound-enabled-speaker-authentication/ultrasound_audio_hu1fb2b889d904d6d5c2d2b18568b844bf_44143_bed9d5ea7508815bc6c3e95dede844ff.JPG&#34;
               width=&#34;611&#34;
               height=&#34;255&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

It shows that when participants say &amp;ldquo;&lt;em&gt;She had your dark suit in greasy wash water all year&lt;/em&gt;&amp;rdquo;, there are some phonemes such as &lt;strong&gt;/sh/&lt;/strong&gt;, &lt;strong&gt;/s/&lt;/strong&gt;, &lt;strong&gt;/sy/&lt;/strong&gt;, &lt;strong&gt;/s/&lt;/strong&gt; reach extremely high frequencies (e.g., ~40kHz). The reason why human can produce some ultrasound is the different manners of articulation make different airstream flows. When two speech organs narrow the airstream to cause friction to occur as it passes through, Fricatives are produced. If the airstream is stopped and then released, Stop or Affricate is produced. We found that &lt;strong&gt;fricative&lt;/strong&gt;, &lt;strong&gt;stop&lt;/strong&gt;, and &lt;strong&gt;affricate&lt;/strong&gt; phonemes often carries more energy in ultrasound band.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Distinctiveness of Ultrasound&lt;/strong&gt;: We extract the speech energies at &lt;strong&gt;fricative&lt;/strong&gt;, &lt;strong&gt;stop&lt;/strong&gt;, and &lt;strong&gt;affricate&lt;/strong&gt; phonemes and summerize them together to represent the identity of the speakers by using $$S_{LTA}(f)=\frac{1}{M}\sum_{t=1}^{N} S(f, p)$$
where &lt;code&gt;S(f,p)&lt;/code&gt; represents the spectrogram value at frequency &lt;code&gt;f&lt;/code&gt; and time frame &lt;code&gt;p&lt;/code&gt;. By comparing the $$S_{LTA}(f)$$ of different participants, we get the following results.
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;distinct&#34; srcset=&#34;
               /talk/ultrasound-enabled-speaker-authentication/distinctive_hu1fb2b889d904d6d5c2d2b18568b844bf_39703_6ffb822d6201c007fc37b1c953210485.JPG 400w,
               /talk/ultrasound-enabled-speaker-authentication/distinctive_hu1fb2b889d904d6d5c2d2b18568b844bf_39703_661751df975620d3e904e2fca6bc33c4.JPG 760w,
               /talk/ultrasound-enabled-speaker-authentication/distinctive_hu1fb2b889d904d6d5c2d2b18568b844bf_39703_1200x1200_fit_q75_lanczos.JPG 1200w&#34;
               src=&#34;https://example.com/talk/ultrasound-enabled-speaker-authentication/distinctive_hu1fb2b889d904d6d5c2d2b18568b844bf_39703_6ffb822d6201c007fc37b1c953210485.JPG&#34;
               width=&#34;625&#34;
               height=&#34;256&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

From the left figure, we can see that participants have dispersed speech energies between frequency range from &lt;code&gt;20kHz-40kHz&lt;/code&gt;, for lower frequencies, we observed similar speech energy trends. The right figure shows the average variance of those speakers, it is evident to show that &lt;code&gt;16-24kHz&lt;/code&gt;, &lt;code&gt;24-32kHz&lt;/code&gt; and &lt;code&gt;32-40kHz&lt;/code&gt; contribute more differences to differentiate participants.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ultrasound Speech for Liveness Detection&lt;/strong&gt; Since we find that human can produce some ultrasound, we then use them to detect whether a sound is generated by human or a speaker.
Through recording the human speech and the replay sound, we present the spectrogram differences as follow: 















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;liveness&#34; srcset=&#34;
               /talk/ultrasound-enabled-speaker-authentication/liveness_hu1fb2b889d904d6d5c2d2b18568b844bf_79038_be60e3c4332c231512b2b7303546573b.JPG 400w,
               /talk/ultrasound-enabled-speaker-authentication/liveness_hu1fb2b889d904d6d5c2d2b18568b844bf_79038_810488cd5cd5a5035e208a06c692da23.JPG 760w,
               /talk/ultrasound-enabled-speaker-authentication/liveness_hu1fb2b889d904d6d5c2d2b18568b844bf_79038_1200x1200_fit_q75_lanczos.JPG 1200w&#34;
               src=&#34;https://example.com/talk/ultrasound-enabled-speaker-authentication/liveness_hu1fb2b889d904d6d5c2d2b18568b844bf_79038_be60e3c4332c231512b2b7303546573b.JPG&#34;
               width=&#34;760&#34;
               height=&#34;192&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Human: Human speech directly recorded by an ultrasound microphone.&lt;/li&gt;
&lt;li&gt;Scenario1: Attackers record and replay with commercial devices (smart phones).&lt;/li&gt;
&lt;li&gt;Scenario2: Attackers record with high-end microphones and replay with commercial speakers (smart phones).&lt;/li&gt;
&lt;li&gt;Scenario3: Attackers record with high-end microphones and replay with ultrasound speakers.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can observe the apparent difference for four spectrograms. By exploiting this clear differences, we can design our liveness detection algorithm to reach high detection accuracy.&lt;/p&gt;
&lt;h3 id=&#34;methodology&#34;&gt;&lt;strong&gt;Methodology&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;model&#34; srcset=&#34;
               /talk/ultrasound-enabled-speaker-authentication/model_hu1fb2b889d904d6d5c2d2b18568b844bf_76798_6930b4fec25a089811e7f4175612b67c.JPG 400w,
               /talk/ultrasound-enabled-speaker-authentication/model_hu1fb2b889d904d6d5c2d2b18568b844bf_76798_697bd31bbe608aa2a97f7ccc4095060a.JPG 760w,
               /talk/ultrasound-enabled-speaker-authentication/model_hu1fb2b889d904d6d5c2d2b18568b844bf_76798_1200x1200_fit_q75_lanczos.JPG 1200w&#34;
               src=&#34;https://example.com/talk/ultrasound-enabled-speaker-authentication/model_hu1fb2b889d904d6d5c2d2b18568b844bf_76798_6930b4fec25a089811e7f4175612b67c.JPG&#34;
               width=&#34;760&#34;
               height=&#34;221&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

We design a end to end speaker authentication system, which includes a liveness detection module and a speaker verification module. By incorporating the ultrasound component in human speech, we design a &lt;em&gt;&lt;strong&gt;Cumulative energy analysis&lt;/strong&gt;&lt;/em&gt; method to detect replay attack. Furthermore, we introduce a &lt;em&gt;&lt;strong&gt;two-stream DNN&lt;/strong&gt;&lt;/em&gt; to handle the low frequency data and the high frequency speech, and fused the extracted feature together to verify the user&amp;rsquo;s voice.&lt;/p&gt;
&lt;h3 id=&#34;demo&#34;&gt;&lt;strong&gt;Demo&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;For more descripitions and &lt;strong&gt;demos&lt;/strong&gt; of our collected high frequency human speech, please visit &lt;a href=&#34;https://supervoiceapp.github.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Here&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Welcome to Wowchemy, the website builder for Hugo</title>
      <link>https://example.com/post/getting-started/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://example.com/post/getting-started/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site&lt;/li&gt;
&lt;li&gt;The template can be modified and customised to suit your needs. It&amp;rsquo;s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a &lt;strong&gt;no-code solution (write in Markdown and customize with YAML parameters)&lt;/strong&gt; and having &lt;strong&gt;flexibility to later add even deeper personalization with HTML and CSS&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more&lt;/li&gt;
&lt;/ol&gt;














&lt;figure  id=&#34;figure-the-template-is-mobile-first-with-a-responsive-design-to-ensure-that-your-site-looks-stunning-on-every-device&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/wowchemy/wowchemy-hugo-modules/master/academic.png&#34; alt=&#34;The template is mobile first with a responsive design to ensure that your site looks stunning on every device.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The template is mobile first with a responsive design to ensure that your site looks stunning on every device.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2 id=&#34;get-started&#34;&gt;Get Started&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;👉 &lt;a href=&#34;https://wowchemy.com/templates/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Create a new site&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;📚 &lt;a href=&#34;https://wowchemy.com/docs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Personalize your site&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;💬 &lt;a href=&#34;https://discord.gg/z8wNYzb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chat with the &lt;strong&gt;Wowchemy community&lt;/strong&gt;&lt;/a&gt; or &lt;a href=&#34;https://discourse.gohugo.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Hugo community&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;🐦 Twitter: &lt;a href=&#34;https://twitter.com/wowchemy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@wowchemy&lt;/a&gt; &lt;a href=&#34;https://twitter.com/guohahaha1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@GeorgeCushen&lt;/a&gt; &lt;a href=&#34;https://twitter.com/search?q=%28%23MadeWithWowchemy%20OR%20%23MadeWithAcademic%29&amp;amp;src=typed_query&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#MadeWithWowchemy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;💡 &lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-modules/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Request a &lt;strong&gt;feature&lt;/strong&gt; or report a &lt;strong&gt;bug&lt;/strong&gt; for &lt;em&gt;Wowchemy&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;⬆️ &lt;strong&gt;Updating Wowchemy?&lt;/strong&gt; View the &lt;a href=&#34;https://wowchemy.com/docs/hugo-tutorials/update/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Update Tutorial&lt;/a&gt; and &lt;a href=&#34;https://wowchemy.com/updates/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Release Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;crowd-funded-open-source-software&#34;&gt;Crowd-funded open-source software&lt;/h2&gt;
&lt;p&gt;To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.&lt;/p&gt;
&lt;h3 id=&#34;-click-here-to-become-a-sponsor-and-help-support-wowchemys-future-httpswowchemycomplans&#34;&gt;&lt;a href=&#34;https://wowchemy.com/plans/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;❤️ Click here to become a sponsor and help support Wowchemy&amp;rsquo;s future ❤️&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;As a token of appreciation for sponsoring, you can &lt;strong&gt;unlock &lt;a href=&#34;https://wowchemy.com/plans/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;these&lt;/a&gt; awesome rewards and extra features 🦄✨&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;ecosystem&#34;&gt;Ecosystem&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/wowchemy/hugo-academic-cli&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugo Academic CLI&lt;/a&gt;:&lt;/strong&gt; Automatically import publications from BibTeX&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;inspiration&#34;&gt;Inspiration&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://academic-demo.netlify.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Check out the latest &lt;strong&gt;demo&lt;/strong&gt;&lt;/a&gt; of what you&amp;rsquo;ll get in less than 10 minutes, or &lt;a href=&#34;https://wowchemy.com/user-stories/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;view the &lt;strong&gt;showcase&lt;/strong&gt;&lt;/a&gt; of personal, project, and business sites.&lt;/p&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Page builder&lt;/strong&gt; - Create &lt;em&gt;anything&lt;/em&gt; with &lt;a href=&#34;https://wowchemy.com/docs/page-builder/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;widgets&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://wowchemy.com/docs/content/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;elements&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Edit any type of content&lt;/strong&gt; - Blog posts, publications, talks, slides, projects, and more!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Create content&lt;/strong&gt; in &lt;a href=&#34;https://wowchemy.com/docs/content/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Markdown&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://wowchemy.com/docs/import/jupyter/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Jupyter&lt;/strong&gt;&lt;/a&gt;, or &lt;a href=&#34;https://wowchemy.com/docs/install-locally/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;RStudio&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Plugin System&lt;/strong&gt; - Fully customizable &lt;a href=&#34;https://wowchemy.com/docs/customization/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;color&lt;/strong&gt; and &lt;strong&gt;font themes&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Display Code and Math&lt;/strong&gt; - Code highlighting and &lt;a href=&#34;https://en.wikibooks.org/wiki/LaTeX/Mathematics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LaTeX math&lt;/a&gt; supported&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Integrations&lt;/strong&gt; - &lt;a href=&#34;https://analytics.google.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Analytics&lt;/a&gt;, &lt;a href=&#34;https://disqus.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Disqus commenting&lt;/a&gt;, Maps, Contact Forms, and more!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Beautiful Site&lt;/strong&gt; - Simple and refreshing one page design&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Industry-Leading SEO&lt;/strong&gt; - Help get your website found on search engines and social media&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Media Galleries&lt;/strong&gt; - Display your images and videos with captions in a customizable gallery&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mobile Friendly&lt;/strong&gt; - Look amazing on every screen with a mobile friendly version of your site&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-language&lt;/strong&gt; - 34+ language packs including English, 中文, and Português&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-user&lt;/strong&gt; - Each author gets their own profile page&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Privacy Pack&lt;/strong&gt; - Assists with GDPR&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stand Out&lt;/strong&gt; - Bring your site to life with animation, parallax backgrounds, and scroll effects&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;One-Click Deployment&lt;/strong&gt; - No servers. No databases. Only files.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;p&gt;Wowchemy and its templates come with &lt;strong&gt;automatic day (light) and night (dark) mode&lt;/strong&gt; built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the &lt;a href=&#34;https://academic-demo.netlify.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Demo&lt;/a&gt; to see it in action! Day/night mode can also be disabled by the site admin in &lt;code&gt;params.toml&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/customization&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Choose a stunning &lt;strong&gt;theme&lt;/strong&gt; and &lt;strong&gt;font&lt;/strong&gt;&lt;/a&gt; for your site. Themes are fully customizable.&lt;/p&gt;
&lt;h2 id=&#34;license&#34;&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright 2016-present &lt;a href=&#34;https://georgecushen.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;George Cushen&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Released under the &lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-modules/blob/master/LICENSE.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIT&lt;/a&gt; license.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning Driven Wireless Real-time Human Activity Recognition</title>
      <link>https://example.com/publication/guo-2020-deep/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/guo-2020-deep/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Surfingattack: Interactive hidden attack on voice assistants using ultrasonic guided waves</title>
      <link>https://example.com/publication/yan-2020-surfingattack/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/yan-2020-surfingattack/</guid>
      <description></description>
    </item>
    
    <item>
      <title>YOLO V3 Implementation with Pytorch &amp; Change Language</title>
      <link>https://example.com/project/yolo/</link>
      <pubDate>Thu, 20 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/yolo/</guid>
      <description>&lt;h2&gt;source code&lt;/h2&gt;
This link behind shows how to implement &lt;a href=&#34;https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/&#34;&gt;YOLOv3 with pytorch instructions&lt;/a&gt;
In my tutorial, I give a solution to convert Label to Chinese Character.
&lt;h2&gt;4 Steps To Convert Label to Chinese&lt;/h2&gt;
&lt;h3&gt;1. Change coconame&lt;/h3&gt;
&lt;p&gt; First of all, we need to translate english label to chinese label
&lt;a href=&#34;https://github.com/hanqingguo/YoloV3Pytorch/blob/master/data/coco.names&#34;&gt;translated labels is availiable here&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;2. Use FreeType to write a script to draw chinese characters&lt;/h3&gt;
&lt;br&gt;
&lt;p&gt; Because Opencv doesn&#39;t support chinese character, we need to use a known font style to draw chinese character, it contains 3 parts:
  &lt;ol&gt;
&lt;li&gt;&lt;code&gt;draw_ft_bitmap(self, img, bitmap, pen, color)&lt;/code&gt;&lt;br&gt;
draw each char &lt;br&gt;
&lt;pre&gt;
    :param bitmap: bitmap
    :param pen:    pen
    :param color:  pen color e.g.(0,0,255) - red
    :return:       image
&lt;/pre&gt;
 &lt;/li&gt;
&lt;li&gt;&lt;code&gt;draw_string(self, img, x_pos, y_pos, text, color)&lt;/code&gt;&lt;br&gt;
  draw string &lt;br&gt;
&lt;pre&gt;
    :param x_pos: text x-postion on img
    :param y_pos: text y-postion on img
    :param text:  text (unicode)
    :param color: text color
    :return:      image
&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;draw_text(self, image, pos, text, text_size, text_color)&lt;/code&gt;&lt;br&gt;
  (draw chinese(or not) text with ttf &lt;br&gt;
&lt;pre&gt;
    :param image:     image(numpy.ndarray) to draw text
    :param pos:       where to draw text
    :param text:      the context, for chinese should be unicode type
    :param text_size: text size
    :param text_color:text color
    :return:          image
&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/p&gt;
&lt;h3&gt;3. Download msyh.ttf&lt;/h3&gt;
&lt;p&gt; .ttf file is dependency to specify font style, it would be loaded in FreeType script, click &lt;a href=&#34;https://github.com/hanqingguo/YoloV3Pytorch/blob/master/msyh.ttf&#34;&gt;here&lt;/a&gt; to Download &lt;/p&gt;
&lt;h3&gt;4. Apply ft2 Chinese Character to yolo&lt;/h3&gt;
&lt;p&gt;In main detect flow (cam_detect.py), instead of using &lt;code&gt;cv2.imshow()&lt;/code&gt;, we use &lt;code&gt;ft.draw_text()&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;
   comment &lt;code&gt;cv2.putText&lt;/code&gt; part, use &lt;code&gt;ft.draw_text&lt;/code&gt; instead &lt;br&gt;
&lt;pre&gt;
    # c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4
    # cv2.rectangle(img, c1, c2,color, -1)
    # cv2.rectangle(img, c1, c2, color, -1)
    # cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 2, [225,255,255], 1);
    ft = ft2.put_chinese_text(&#39;msyh.ttf&#39;)
    ft.draw_text(image=img, pos=(c1[0], c1[1] + t_size[1] - 7), text=label, text_size=15, text_color=[255, 255, 255])
&lt;/pre&gt;&lt;/p&gt;
&lt;h3&gt;Before Translate&lt;/h3&gt;
&lt;video width=&#34;400&#34; controls&gt;
&lt;source src=&#34;eng_caption.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;br/&gt;
&lt;h3&gt;After Translate&lt;/h3&gt;
&lt;video width=&#34;400&#34; controls&gt;
&lt;source src=&#34;cn_caption.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;/br&gt;
&lt;p&gt;You can Download All source code from my github,&lt;a href=&#34;https://github.com/hanqingguo/YoloV3Pytorch&#34;&gt;here&lt;/a&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://example.com/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://example.com/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-wowchemy&#34;&gt;Create slides in Markdown with Wowchemy&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wowchemy&lt;/a&gt; | &lt;a href=&#34;https://owchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;span class=&#34;fragment &#34; &gt;
   One 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   **Two** 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   Three 
&lt;/span&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/media/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-modules/discussions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Automated labeling for robotic autonomous navigation through multi-sensory semi-supervised learning on big data</title>
      <link>https://example.com/publication/xu-2019-automated/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/xu-2019-automated/</guid>
      <description></description>
    </item>
    
    <item>
      <title>DSIC: Deep learning based self-interference cancellation for in-band full duplex wireless</title>
      <link>https://example.com/publication/guo-2019-dsic/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/guo-2019-dsic/</guid>
      <description></description>
    </item>
    
    <item>
      <title>In-band full duplex wireless communications and networking for iot devices: Progress, challenges and opportunities</title>
      <link>https://example.com/publication/wu-2019-band/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/wu-2019-band/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Real-time human activity recognition based on radar</title>
      <link>https://example.com/publication/guo-2019-real/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/guo-2019-real/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A deep residual convolutional neural network for facial keypoint detection with missing labels</title>
      <link>https://example.com/publication/wu-2018-deep/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/wu-2018-deep/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Distance based user localization and tracking with mechanical ultrasonic beamforming</title>
      <link>https://example.com/publication/zhu-2018-distance/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/zhu-2018-distance/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Indoor human activity recognition based on ambient radar with signal processing and machine learning</title>
      <link>https://example.com/publication/zhu-2018-indoor/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/zhu-2018-indoor/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Indoor multi-sensory self-supervised autonomous mobile robotic navigation</title>
      <link>https://example.com/publication/xu-2018-indoor/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/xu-2018-indoor/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Non-contact non-invasive heart and respiration rates monitoring with MIMO radar sensing</title>
      <link>https://example.com/publication/liu-2018-non/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/liu-2018-non/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Real Time 3D Indoor Human Image Capturing Based on FMCW Radar</title>
      <link>https://example.com/publication/guo-2018-real/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/guo-2018-real/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Realtime software defined self-interference cancellation based on machine learning for in-band full duplex wireless communications</title>
      <link>https://example.com/publication/guo-2018-realtime/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/guo-2018-realtime/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Removing Background with Semantic Segmentation Based on Ensemble Learning</title>
      <link>https://example.com/publication/xu-2018-removing/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/xu-2018-removing/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Avoidance of manual labeling in robotic autonomous navigation through multi-sensory semi-supervised learning</title>
      <link>https://example.com/publication/xu-2017-avoidance/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/xu-2017-avoidance/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Real Time Video Stitching by Exploring Temporal and Spatial Features</title>
      <link>https://example.com/publication/wu-2017-real/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/wu-2017-real/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Deepfakes Implementation with Pytorch</title>
      <link>https://example.com/project/deepfake/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/deepfake/</guid>
      <description>&lt;h2&gt;source code&lt;/h2&gt;
This link behind shows how to implement &lt;a href=&#34;https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/&#34;&gt;YOLOv3 with pytorch instructions&lt;/a&gt;
In my tutorial, I give a solution to convert Label to Chinese Character.
&lt;h2&gt;4 Steps To Convert Label to Chinese&lt;/h2&gt;
&lt;h3&gt;1. Change coconame&lt;/h3&gt;
&lt;p&gt; First of all, we need to translate english label to chinese label
&lt;a href=&#34;https://github.com/hanqingguo/YoloV3Pytorch/blob/master/data/coco.names&#34;&gt;translated labels is availiable here&lt;/a&gt;&lt;/p&gt;
&lt;br&gt;
&lt;h3&gt;2. Use FreeType to write a script to draw chinese characters&lt;/h3&gt;
&lt;br&gt;
&lt;p&gt; Because Opencv doesn&#39;t support chinese character, we need to use a known font style to draw chinese character, it contains 3 parts:
  &lt;ol&gt;
&lt;li&gt;&lt;code&gt;draw_ft_bitmap(self, img, bitmap, pen, color)&lt;/code&gt;&lt;br&gt;
draw each char &lt;br&gt;
&lt;pre&gt;
    :param bitmap: bitmap
    :param pen:    pen
    :param color:  pen color e.g.(0,0,255) - red
    :return:       image
&lt;/pre&gt;
 &lt;/li&gt;
&lt;li&gt;&lt;code&gt;draw_string(self, img, x_pos, y_pos, text, color)&lt;/code&gt;&lt;br&gt;
  draw string &lt;br&gt;
&lt;pre&gt;
    :param x_pos: text x-postion on img
    :param y_pos: text y-postion on img
    :param text:  text (unicode)
    :param color: text color
    :return:      image
&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;draw_text(self, image, pos, text, text_size, text_color)&lt;/code&gt;&lt;br&gt;
  (draw chinese(or not) text with ttf &lt;br&gt;
&lt;pre&gt;
    :param image:     image(numpy.ndarray) to draw text
    :param pos:       where to draw text
    :param text:      the context, for chinese should be unicode type
    :param text_size: text size
    :param text_color:text color
    :return:          image
&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/p&gt;
&lt;h3&gt;3. Download msyh.ttf&lt;/h3&gt;
&lt;p&gt; .ttf file is dependency to specify font style, it would be loaded in FreeType script, click &lt;a href=&#34;https://github.com/hanqingguo/YoloV3Pytorch/blob/master/msyh.ttf&#34;&gt;here&lt;/a&gt; to Download &lt;/p&gt;
&lt;h3&gt;4. Apply ft2 Chinese Character to yolo&lt;/h3&gt;
&lt;p&gt;In main detect flow (cam_detect.py), instead of using &lt;code&gt;cv2.imshow()&lt;/code&gt;, we use &lt;code&gt;ft.draw_text()&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;
   comment &lt;code&gt;cv2.putText&lt;/code&gt; part, use &lt;code&gt;ft.draw_text&lt;/code&gt; instead &lt;br&gt;
&lt;pre&gt;
    # c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4
    # cv2.rectangle(img, c1, c2,color, -1)
    # cv2.rectangle(img, c1, c2, color, -1)
    # cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 2, [225,255,255], 1);
    ft = ft2.put_chinese_text(&#39;msyh.ttf&#39;)
    ft.draw_text(image=img, pos=(c1[0], c1[1] + t_size[1] - 7), text=label, text_size=15, text_color=[255, 255, 255])
&lt;/pre&gt;&lt;/p&gt;
&lt;h3&gt;Before Translate&lt;/h3&gt;
&lt;video width=&#34;400&#34; controls&gt;
&lt;source src=&#34;https://example.com/assets/media/yolo/eng_caption.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;br/&gt;
&lt;h3&gt;After Translate&lt;/h3&gt;
&lt;video width=&#34;400&#34; controls&gt;
&lt;source src=&#34;https://example.com/assets/media/yolo/cn_caption.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;/br&gt;
&lt;p&gt;You can Download All source code from my github,&lt;a href=&#34;https://github.com/hanqingguo/YoloV3Pytorch&#34;&gt;here&lt;/a&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Walabot Shows 3D Image</title>
      <link>https://example.com/project/radar/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/radar/</guid>
      <description>&lt;style&gt;
.column {
  float: left;
  width: 50%;
  padding: 5px;
}

/* Clear floats after image containers */
.row::after {
  content: &#34;&#34;;
  clear: both;
  display: table;
}
&lt;/style&gt;
&lt;p&gt;&lt;a href=&#34;https://walabot.com/&#34;&gt;Walabot&lt;/a&gt; senses environment by transmitting, receiving and recording signals from MIMO antennas. The frequency range from 3.3-10 GHz.
Today I am going to show you how walabot collect 3D images.&lt;/p&gt;
&lt;h2&gt;Raw signals&lt;/h2&gt;
  &lt;img src=&#34;raw.PNG&#34; width=&#34;50%&#34;/&gt;
  &lt;br/&gt;
  &lt;p&gt;
  Our radar sensing platform emits probing pulse signals x(t) at a pulse repetition frequency (PRF) of 16 Hz, but within each pulse repetition interval (PRI), the receiver antenna
samples the received signal y(t) at a very high frequency of 8 KHz.
  The x-axis of raw signals is response time, while y-axis means amplitude at that time slot. Red line is actually samples what receiver antenna sampling.
  In other words, response time is signal traverse time from transmit antenna to receive antenna, which depends on distance to radar. Higher amplitude at specific time, means there is object at that place.
  &lt;/p&gt;
&lt;h2&gt;2D images&lt;/h2&gt;
&lt;div class=&#34;row&#34;&gt;
  &lt;div class=&#34;column&#34;&gt;
    &lt;img src=&#34;2d.PNG&#34; style=&#34;width:100%&#34;&gt;
  &lt;/div&gt;
  &lt;div class=&#34;column&#34;&gt;
    &lt;img src=&#34;2d-real.png&#34; height=60% style=&#34;width:100%&#34;&gt;
  &lt;/div&gt;
&lt;/div&gt;
  &lt;p&gt;While 2D images only shows &amp;phi; (wide angle) versus &lt;var&gt;R&lt;/var&gt;;. However, any object in real world is 3D, it has height as well, then introduce &amp;theta; (elevation angle) as height.
  To get 3D image, let&#39;s see the axis system in walabot.&lt;/p&gt;
    &lt;img src=&#34;axis.png&#34; width=&#34;50%&#34; height=&#34;50%&#34;/&gt;&lt;br&gt;
    where &lt;br&gt;
    &lt;code&gt;
      X = R*Sin&amp;theta;&lt;br&gt;
      Y = R*Cos&amp;theta;Sin&amp;phi;&lt;br&gt;
      Z = R*Cos&amp;theta;Cos&amp;phi;
    &lt;/code&gt;
&lt;h2&gt;3D images&lt;/h2&gt;
  &lt;p&gt;Instead of using 2D images, we construct 3D images based on those 2D images by stacking them in vertical direction.&lt;/p&gt;
  &lt;img src=&#34;3d-stack.png&#34; width=&#34;85%&#34;/&gt;
  &lt;p&gt;Figure above shows how to stack 2D images, the implement process are
    &lt;ol&gt;
  &lt;li&gt;Concatenate 2D images to 3D matrix &lt;/li&gt;
  &lt;li&gt;Use &lt;code&gt;measure.marching_cubes_classic&lt;/code&gt; to make vertices and faces&lt;/li&gt;
  &lt;li&gt;Change axis ranges from interval index to real unit&lt;br&gt;
    For example: &lt;br&gt;
    &lt;code&gt;
    (0,100) =&gt; (1,200)  R(cm)&lt;br&gt;
    (0,61)  =&gt; (-90,90) &amp;phi;(degree)&lt;br&gt;
    (0,9)   =&gt; (-20,20) &amp;theta;(degree)
    &lt;/code&gt;
  &lt;/li&gt;
  &lt;li&gt;Use &lt;code&gt;Poly3DCollection&lt;/code&gt; to get mesh&lt;/li&gt;
&lt;/ol&gt;
  &lt;/p&gt;
&lt;h2&gt;3D videos&lt;/h2&gt;
  &lt;p&gt;Once get 3D image, we save it to IO buffer, and use PIL to open buffer, then convert it to ndarray, write ndarray as one frame to video by using openCV&lt;/p&gt;
  &lt;video width=&#34;400&#34; controls&gt;
  &lt;source src=&#34;walk.mp4&#34; type=&#34;video/mp4&#34;&gt;
  &lt;/video&gt;
  &lt;p&gt;Video above shows a human walk around walabot radar.&lt;/p&gt;
&lt;h2&gt;CNN extract Features&lt;/h2&gt;
&lt;p&gt;For each frame, we use resnet-18 to extract features, we change last Average pooling to Max pooling because Max pooling extracts the most important features like edges whereas,
  average pooling extracts features so smoothly. For image data, you can see the difference. Although both are used for same reason, I think max pooling is better for extracting the extreme features.
  Average pooling sometimes can’t extract good features because it takes all into count and results an average value which may/may not be important for object detection type tasks.
  Then change output linear layer to extract 10 features.
&lt;/p&gt;
&lt;h2&gt;LSTM training&lt;/h2&gt;
  &lt;p&gt;&lt;img src=&#34;lstm.png&#34; width=&#34;45%&#34; height=&#34;60%&#34;/&gt;&lt;br&gt;
    &lt;br&gt;
    We use 3 frames to recognize one activity. Since 3D radar signal shown above is abstract video. Unlike camera videos which each frame represent a activity, radar video can only be detected by
    continuous frames change to recognize one activity. We collect many 3 frames video for each activity as training data. When testing, given continuous stream radar video, and feed every 3 frames to
    LSTM network, then gives our prediction for each frame.
    Detailed technique will be present in my next paper.
  &lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://example.com/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.com/admin/config.yml</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://example.com/publication/example/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/example/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
