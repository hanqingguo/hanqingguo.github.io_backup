<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Academic</title>
    <link>https://example.com/</link>
      <atom:link href="https://example.com/index.xml" rel="self" type="application/rss+xml" />
    <description>Academic</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 01 Jan 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://example.com/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Academic</title>
      <link>https://example.com/</link>
    </image>
    
    <item>
      <title>Black-Box, Query-Efficient Audio Adversarial Attacks</title>
      <link>https://example.com/talk/black-box-query-efficient-audio-adversarial-attacks/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://example.com/talk/black-box-query-efficient-audio-adversarial-attacks/</guid>
      <description>&lt;h3 id=&#34;motivation&#34;&gt;&lt;strong&gt;Motivation&lt;/strong&gt;:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Query-Inefficient&lt;/strong&gt;: Existing black box audio attacks require plenty of queries to interact with the victim speech models, e.g., &lt;a href=&#34;https://www.usenix.org/system/files/sec20summer_chen-yuxuan_prepub.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Devil&amp;rsquo;s whisper&lt;/a&gt; attacks commercial speech to text model by training an substitute model through querying victim model. &lt;a href=&#34;https://arxiv.org/pdf/2110.09714.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OCCAM&lt;/a&gt; estimates the gradient by incorporating evolutionary algorithms. The extensive query is time consuming, costly and may attract attention.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Decision-Boundary&lt;/strong&gt;: We find that the decision boundary in speech to text model is different from it in computer vision models. Due to the non-contiguous decision boundary, it&amp;rsquo;s hard for attacker to initialize the perturbation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Phoneme&lt;/strong&gt; We find a short phoneme can surprisingly alter the speech model prediction. Based on the observation, we decide to optimize the duration and power of phoneme to inject it attacking the speech to text model.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;methodology&#34;&gt;&lt;strong&gt;Methodology&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;method&#34; srcset=&#34;
               /talk/black-box-query-efficient-audio-adversarial-attacks/method_hu1fb2b889d904d6d5c2d2b18568b844bf_65740_09e22208c629ca2bc018ec837b2af644.JPG 400w,
               /talk/black-box-query-efficient-audio-adversarial-attacks/method_hu1fb2b889d904d6d5c2d2b18568b844bf_65740_c2c486aed395a6a05cadf6cffa5ca2b3.JPG 760w,
               /talk/black-box-query-efficient-audio-adversarial-attacks/method_hu1fb2b889d904d6d5c2d2b18568b844bf_65740_1200x1200_fit_q75_lanczos.JPG 1200w&#34;
               src=&#34;https://example.com/talk/black-box-query-efficient-audio-adversarial-attacks/method_hu1fb2b889d904d6d5c2d2b18568b844bf_65740_09e22208c629ca2bc018ec837b2af644.JPG&#34;
               width=&#34;760&#34;
               height=&#34;250&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
 There are three stages to generate a perturbation. At stage 1, given an input audio, we inject many different phonemes to it and try to alter the transcript result. Next, we collect the phonemes or phoneme combinations to a perturbation set. At stage 3, we estimate the gradient by applying small changes to the candidate perturbation, and then estimate the gradient direction through observing the prediction result changes. Then apply the estimated gradient to fine tuning the perturbation. Finally, we can get the adversarial perturbation targeting to a input audio.&lt;/p&gt;
&lt;h3 id=&#34;demo&#34;&gt;&lt;strong&gt;Demo&lt;/strong&gt;&lt;/h3&gt;
&lt;div&gt;
&lt;table&gt;
&lt;tr&gt;
	&lt;td&gt;Bob and Alice talk simultaneously&lt;/td&gt;
	&lt;td&gt;Only Alice&#39;s voice is kept&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
	&lt;td&gt;&lt;audio controls&gt;
		  &lt;source src=&#34;./assets/joint.wav&#34; type=&#34;audio/wav&#34;&gt;
		  Your browser does not support the &lt;code&gt;audio&lt;/code&gt; element.
		&lt;/audio&gt;&lt;/td&gt;
	&lt;td&gt;&lt;audio controls&gt;
		  &lt;source src=&#34;./assets/joint-focus.wav&#34; type=&#34;audio/wav&#34;&gt;
		  Your browser does not support the &lt;code&gt;audio&lt;/code&gt; element.
		&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>NELoRa: Towards Ultra-low SNR LoRa Communication with Neural-enhanced Demodulation</title>
      <link>https://example.com/publication/li-2021-nelora/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/li-2021-nelora/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Rectifying Administrated ERC20 Tokens</title>
      <link>https://example.com/publication/ivanov-2021-rectifying/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/ivanov-2021-rectifying/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Speaker Selective Cancellation via Neural Enhanced Ultrasound Shadowing</title>
      <link>https://example.com/talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://example.com/talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/</guid>
      <description>&lt;h3 id=&#34;motivation&#34;&gt;&lt;strong&gt;Motivation&lt;/strong&gt;:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Necessity of Recorder Jammer:&lt;/strong&gt; Voice recording is an essential information-sharing approach, which is benefiting many aspects of our daily life. Nowadays, smartphones and Internet-of-Things (IoT) devices equipped with microphones allow people to record voice anytime and anywhere.
However, the growing presence of unauthorized microphones has led to numerous incidences of privacy violations. Off-the-shelf microphones are widely available and can be deployed to steal users&#39; biometric traits (e.g. voiceprints) or private conversations. Thus, unauthorized voice recording has become a serious societal issue.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Limitation of Existing Jammer:&lt;/strong&gt; Existing jammers disrupt unauthorized voice recording by emiting an ultrasounic scrambling noise. However, there are two limitations of those applications/devices.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It will &lt;strong&gt;affact&lt;/strong&gt; the normal usage of other voice users. For example, in the cover figure, when Bob deploys such jammer in public, all the microphones will be jammed, which means the surrounded people cannnot use their voice assistants, video call, and enmergency call meanwhile.&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s &lt;strong&gt;not secure enough&lt;/strong&gt;. The jammed recording can be parsed if the attacker know the noise pattern. If the attacker learns the frequency pattern of the scrambling noise wave, the attacker can deploy an additional microphone to nullify the noises and record them illegally&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Speaker specified feature&lt;/strong&gt;: We found every speaker has a distinct speech feature that introduced by the variety of their vocal system. By adopting the speaker specified feature, we can extract Bob&amp;rsquo;s voice from a mixed audio input. Then we use a ultrasound speaker to play the rest sound (e.g., Alice&amp;rsquo;s voice) that not belong to Bob, and overshadow Bob&amp;rsquo;s voice in the wild while maintaining the other speakers&#39; voice on their end.
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;demo&#34; srcset=&#34;
               /talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/demo_hu1fb2b889d904d6d5c2d2b18568b844bf_39706_27d441b62bd99f6b05ccf1d5ad59f13b.JPG 400w,
               /talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/demo_hu1fb2b889d904d6d5c2d2b18568b844bf_39706_f124db61864ebc361caff305f8256440.JPG 760w,
               /talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/demo_hu1fb2b889d904d6d5c2d2b18568b844bf_39706_1200x1200_fit_q75_lanczos.JPG 1200w&#34;
               src=&#34;https://example.com/talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/demo_hu1fb2b889d904d6d5c2d2b18568b844bf_39706_27d441b62bd99f6b05ccf1d5ad59f13b.JPG&#34;
               width=&#34;603&#34;
               height=&#34;346&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;methodology&#34;&gt;&lt;strong&gt;Methodology&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/model1_hu1fb2b889d904d6d5c2d2b18568b844bf_50310_ff684e1ceeec991546c5270752b587ef.JPG 400w,
               /talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/model1_hu1fb2b889d904d6d5c2d2b18568b844bf_50310_e3e4ff91bc47c51a524610e7db1627c2.JPG 760w,
               /talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/model1_hu1fb2b889d904d6d5c2d2b18568b844bf_50310_1200x1200_fit_q75_lanczos.JPG 1200w&#34;
               src=&#34;https://example.com/talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/model1_hu1fb2b889d904d6d5c2d2b18568b844bf_50310_ff684e1ceeec991546c5270752b587ef.JPG&#34;
               width=&#34;666&#34;
               height=&#34;268&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;br&gt;
The core of our model is the speaker &lt;strong&gt;Selector&lt;/strong&gt; that present in above figure. We use the &lt;strong&gt;Selector&lt;/strong&gt; to extract Bob&amp;rsquo;s voice in real-time. To train the &lt;strong&gt;Selector&lt;/strong&gt;, we construct many mixed audio from two speakers, and introduce the prior knowledge (&lt;strong&gt;Reference Audio&lt;/strong&gt;) of the one speaker in the &lt;strong&gt;mixed audio&lt;/strong&gt;. Through filtering out the known speaker from the mixed audio, the &lt;strong&gt;Selector&lt;/strong&gt; learns to extract speaker specified voice from the mixed sound. Then, the &lt;strong&gt;Selector&lt;/strong&gt; produces the filtered &lt;strong&gt;Bob&amp;rsquo;s irrelated&lt;/strong&gt; to mix the original sound, and hence hide &lt;strong&gt;Bob&amp;rsquo;s voice&lt;/strong&gt; on the recorder side.&lt;/p&gt;
&lt;h3 id=&#34;demo&#34;&gt;&lt;strong&gt;Demo&lt;/strong&gt;&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Setup-1&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Setup-2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/setup_hu1fb2b889d904d6d5c2d2b18568b844bf_51378_c7e15a61a883c5603ca9d6f540faf8eb.JPG 400w,
               /talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/setup_hu1fb2b889d904d6d5c2d2b18568b844bf_51378_39524779f4b62b91a5761ab8b9f62aa1.JPG 760w,
               /talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/setup_hu1fb2b889d904d6d5c2d2b18568b844bf_51378_1200x1200_fit_q75_lanczos.JPG 1200w&#34;
               src=&#34;https://example.com/talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/setup_hu1fb2b889d904d6d5c2d2b18568b844bf_51378_c7e15a61a883c5603ca9d6f540faf8eb.JPG&#34;
               width=&#34;483&#34;
               height=&#34;347&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/setup2_hu1fb2b889d904d6d5c2d2b18568b844bf_36073_1c526fbced878306993efd729fb86483.JPG 400w,
               /talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/setup2_hu1fb2b889d904d6d5c2d2b18568b844bf_36073_9c49360bdf827aacbaf02e4544fd1231.JPG 760w,
               /talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/setup2_hu1fb2b889d904d6d5c2d2b18568b844bf_36073_1200x1200_fit_q75_lanczos.JPG 1200w&#34;
               src=&#34;https://example.com/talk/speaker-selective-cancellation-via-neural-enhanced-ultrasound-shadowing/setup2_hu1fb2b889d904d6d5c2d2b18568b844bf_36073_1c526fbced878306993efd729fb86483.JPG&#34;
               width=&#34;443&#34;
               height=&#34;376&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;There are two experimental settings of our work. &lt;strong&gt;Left&lt;/strong&gt; is the benchmark test running with public dataset and simulate the mixed speaker plays the mixed audio. &lt;strong&gt;Right&lt;/strong&gt; is the real world scenario that Bob holds our device to protect his voice without intervening Alice&amp;rsquo;s phone use.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Below are the voice demos of our system that recorded from &lt;strong&gt;Alice&amp;rsquo;s recorder&lt;/strong&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;div&gt;
&lt;table&gt;
&lt;tr&gt;
	&lt;td&gt;Bob and Alice talk simultaneously&lt;/td&gt;
	&lt;td&gt;Only Alice&#39;s voice is kept&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
	&lt;td&gt;&lt;audio controls&gt;
		  &lt;source src=&#34;joint.wav&#34; type=&#34;audio/wav&#34;&gt;
		  Your browser does not support the &lt;code&gt;audio&lt;/code&gt; element.
		&lt;/audio&gt;&lt;/td&gt;
	&lt;td&gt;&lt;audio controls&gt;
		  &lt;source src=&#34;joint-focus.wav&#34; type=&#34;audio/wav&#34;&gt;
		  Your browser does not support the &lt;code&gt;audio&lt;/code&gt; element.
		&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
	&lt;td&gt;&lt;audio controls&gt;
		  &lt;source src=&#34;conversation.wav&#34; type=&#34;audio/wav&#34;&gt;
		  Your browser does not support the &lt;code&gt;audio&lt;/code&gt; element.
		&lt;/audio&gt;&lt;/td&gt;
	&lt;td&gt;&lt;audio controls&gt;
		  &lt;source src=&#34;conversation-hide.wav&#34; type=&#34;audio/wav&#34;&gt;
		  Your browser does not support the &lt;code&gt;audio&lt;/code&gt; element.
		&lt;/audio&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Ultrasound Enabled Speaker Authentication</title>
      <link>https://example.com/talk/ultrasound-enabled-speaker-authentication/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://example.com/talk/ultrasound-enabled-speaker-authentication/</guid>
      <description>&lt;h3 id=&#34;motivation&#34;&gt;&lt;strong&gt;Motivation&lt;/strong&gt;:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ultrasound in Speech&lt;/strong&gt;: We found that human can produce ultrasound by collecting human speech with a high-end ultrasonic microphone (&lt;a href=&#34;http://www.avisoft.com/ultrasound-microphones/cm16-cmpa/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CM-16&lt;/a&gt;). See the experimental setup below:
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;setup&#34; srcset=&#34;
               /talk/ultrasound-enabled-speaker-authentication/testbed_hu1fb2b889d904d6d5c2d2b18568b844bf_60110_7651c9548bc1480dd37f1c5edab33133.JPG 400w,
               /talk/ultrasound-enabled-speaker-authentication/testbed_hu1fb2b889d904d6d5c2d2b18568b844bf_60110_8257df6de9f21ab2496f1da5da7bf73e.JPG 760w,
               /talk/ultrasound-enabled-speaker-authentication/testbed_hu1fb2b889d904d6d5c2d2b18568b844bf_60110_1200x1200_fit_q75_lanczos.JPG 1200w&#34;
               src=&#34;https://example.com/talk/ultrasound-enabled-speaker-authentication/testbed_hu1fb2b889d904d6d5c2d2b18568b844bf_60110_7651c9548bc1480dd37f1c5edab33133.JPG&#34;
               width=&#34;634&#34;
               height=&#34;385&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
 From the recorded human speech, we analyze the spectrogram and present it to the next figure.
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;method&#34; srcset=&#34;
               /talk/ultrasound-enabled-speaker-authentication/ultrasound_audio_hu1fb2b889d904d6d5c2d2b18568b844bf_44143_bed9d5ea7508815bc6c3e95dede844ff.JPG 400w,
               /talk/ultrasound-enabled-speaker-authentication/ultrasound_audio_hu1fb2b889d904d6d5c2d2b18568b844bf_44143_750aafde5cd2e7168ccbb0118d2239a3.JPG 760w,
               /talk/ultrasound-enabled-speaker-authentication/ultrasound_audio_hu1fb2b889d904d6d5c2d2b18568b844bf_44143_1200x1200_fit_q75_lanczos.JPG 1200w&#34;
               src=&#34;https://example.com/talk/ultrasound-enabled-speaker-authentication/ultrasound_audio_hu1fb2b889d904d6d5c2d2b18568b844bf_44143_bed9d5ea7508815bc6c3e95dede844ff.JPG&#34;
               width=&#34;611&#34;
               height=&#34;255&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

It shows that when participants say &amp;ldquo;&lt;em&gt;She had your dark suit in greasy wash water all year&lt;/em&gt;&amp;rdquo;, there are some phonemes such as &lt;strong&gt;/sh/&lt;/strong&gt;, &lt;strong&gt;/s/&lt;/strong&gt;, &lt;strong&gt;/sy/&lt;/strong&gt;, &lt;strong&gt;/s/&lt;/strong&gt; reach extremely high frequencies (e.g., ~40kHz). The reason why human can produce some ultrasound is the different manners of articulation make different airstream flows. When two speech organs narrow the airstream to cause friction to occur as it passes through, Fricatives are produced. If the airstream is stopped and then released, Stop or Affricate is produced. We found that &lt;strong&gt;fricative&lt;/strong&gt;, &lt;strong&gt;stop&lt;/strong&gt;, and &lt;strong&gt;affricate&lt;/strong&gt; phonemes often carries more energy in ultrasound band.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Distinctiveness of Ultrasound&lt;/strong&gt;: We extract the speech energies at &lt;strong&gt;fricative&lt;/strong&gt;, &lt;strong&gt;stop&lt;/strong&gt;, and &lt;strong&gt;affricate&lt;/strong&gt; phonemes and summerize them together to represent the identity of the speakers by using $$S_{LTA}(f)=\frac{1}{M}\sum_{t=1}^{N} S(f, p)$$
where &lt;code&gt;S(f,p)&lt;/code&gt; represents the spectrogram value at frequency &lt;code&gt;f&lt;/code&gt; and time frame &lt;code&gt;p&lt;/code&gt;. By comparing the $$S_{LTA}(f)$$ of different participants, we get the following results.
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;distinct&#34; srcset=&#34;
               /talk/ultrasound-enabled-speaker-authentication/distinctive_hu1fb2b889d904d6d5c2d2b18568b844bf_39703_6ffb822d6201c007fc37b1c953210485.JPG 400w,
               /talk/ultrasound-enabled-speaker-authentication/distinctive_hu1fb2b889d904d6d5c2d2b18568b844bf_39703_661751df975620d3e904e2fca6bc33c4.JPG 760w,
               /talk/ultrasound-enabled-speaker-authentication/distinctive_hu1fb2b889d904d6d5c2d2b18568b844bf_39703_1200x1200_fit_q75_lanczos.JPG 1200w&#34;
               src=&#34;https://example.com/talk/ultrasound-enabled-speaker-authentication/distinctive_hu1fb2b889d904d6d5c2d2b18568b844bf_39703_6ffb822d6201c007fc37b1c953210485.JPG&#34;
               width=&#34;625&#34;
               height=&#34;256&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

From the left figure, we can see that participants have dispersed speech energies between frequency range from &lt;code&gt;20kHz-40kHz&lt;/code&gt;, for lower frequencies, we observed similar speech energy trends. The right figure shows the average variance of those speakers, it is evident to show that &lt;code&gt;16-24kHz&lt;/code&gt;, &lt;code&gt;24-32kHz&lt;/code&gt; and &lt;code&gt;32-40kHz&lt;/code&gt; contribute more differences to differentiate participants.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ultrasound Speech for Liveness Detection&lt;/strong&gt; Since we find that human can produce some ultrasound, we then use them to detect whether a sound is generated by human or a speaker.
Through recording the human speech and the replay sound, we present the spectrogram differences as follow: 















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;liveness&#34; srcset=&#34;
               /talk/ultrasound-enabled-speaker-authentication/liveness_hu1fb2b889d904d6d5c2d2b18568b844bf_79038_be60e3c4332c231512b2b7303546573b.JPG 400w,
               /talk/ultrasound-enabled-speaker-authentication/liveness_hu1fb2b889d904d6d5c2d2b18568b844bf_79038_810488cd5cd5a5035e208a06c692da23.JPG 760w,
               /talk/ultrasound-enabled-speaker-authentication/liveness_hu1fb2b889d904d6d5c2d2b18568b844bf_79038_1200x1200_fit_q75_lanczos.JPG 1200w&#34;
               src=&#34;https://example.com/talk/ultrasound-enabled-speaker-authentication/liveness_hu1fb2b889d904d6d5c2d2b18568b844bf_79038_be60e3c4332c231512b2b7303546573b.JPG&#34;
               width=&#34;760&#34;
               height=&#34;192&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Human: Human speech directly recorded by an ultrasound microphone.&lt;/li&gt;
&lt;li&gt;Scenario1: Attackers record and replay with commercial devices (smart phones).&lt;/li&gt;
&lt;li&gt;Scenario2: Attackers record with high-end microphones and replay with commercial speakers (smart phones).&lt;/li&gt;
&lt;li&gt;Scenario3: Attackers record with high-end microphones and replay with ultrasound speakers.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can observe the apparent difference for four spectrograms. By exploiting this clear differences, we can design our liveness detection algorithm to reach high detection accuracy.&lt;/p&gt;
&lt;h3 id=&#34;methodology&#34;&gt;&lt;strong&gt;Methodology&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;model&#34; srcset=&#34;
               /talk/ultrasound-enabled-speaker-authentication/model_hu1fb2b889d904d6d5c2d2b18568b844bf_76798_6930b4fec25a089811e7f4175612b67c.JPG 400w,
               /talk/ultrasound-enabled-speaker-authentication/model_hu1fb2b889d904d6d5c2d2b18568b844bf_76798_697bd31bbe608aa2a97f7ccc4095060a.JPG 760w,
               /talk/ultrasound-enabled-speaker-authentication/model_hu1fb2b889d904d6d5c2d2b18568b844bf_76798_1200x1200_fit_q75_lanczos.JPG 1200w&#34;
               src=&#34;https://example.com/talk/ultrasound-enabled-speaker-authentication/model_hu1fb2b889d904d6d5c2d2b18568b844bf_76798_6930b4fec25a089811e7f4175612b67c.JPG&#34;
               width=&#34;760&#34;
               height=&#34;221&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

We design a end to end speaker authentication system, which includes a liveness detection module and a speaker verification module. By incorporating the ultrasound component in human speech, we design a &lt;em&gt;&lt;strong&gt;Cumulative energy analysis&lt;/strong&gt;&lt;/em&gt; method to detect replay attack. Furthermore, we introduce a &lt;em&gt;&lt;strong&gt;two-stream DNN&lt;/strong&gt;&lt;/em&gt; to handle the low frequency data and the high frequency speech, and fused the extracted feature together to verify the user&amp;rsquo;s voice.&lt;/p&gt;
&lt;h3 id=&#34;demo&#34;&gt;&lt;strong&gt;Demo&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;For more descripitions and &lt;strong&gt;demos&lt;/strong&gt; of our collected high frequency human speech, please visit &lt;a href=&#34;https://supervoiceapp.github.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Here&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Welcome to Wowchemy, the website builder for Hugo</title>
      <link>https://example.com/post/getting-started/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://example.com/post/getting-started/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site&lt;/li&gt;
&lt;li&gt;The template can be modified and customised to suit your needs. It&amp;rsquo;s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a &lt;strong&gt;no-code solution (write in Markdown and customize with YAML parameters)&lt;/strong&gt; and having &lt;strong&gt;flexibility to later add even deeper personalization with HTML and CSS&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more&lt;/li&gt;
&lt;/ol&gt;














&lt;figure  id=&#34;figure-the-template-is-mobile-first-with-a-responsive-design-to-ensure-that-your-site-looks-stunning-on-every-device&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/wowchemy/wowchemy-hugo-modules/master/academic.png&#34; alt=&#34;The template is mobile first with a responsive design to ensure that your site looks stunning on every device.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The template is mobile first with a responsive design to ensure that your site looks stunning on every device.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2 id=&#34;get-started&#34;&gt;Get Started&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;👉 &lt;a href=&#34;https://wowchemy.com/templates/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Create a new site&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;📚 &lt;a href=&#34;https://wowchemy.com/docs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Personalize your site&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;💬 &lt;a href=&#34;https://discord.gg/z8wNYzb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chat with the &lt;strong&gt;Wowchemy community&lt;/strong&gt;&lt;/a&gt; or &lt;a href=&#34;https://discourse.gohugo.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Hugo community&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;🐦 Twitter: &lt;a href=&#34;https://twitter.com/wowchemy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@wowchemy&lt;/a&gt; &lt;a href=&#34;https://twitter.com/GeorgeCushen&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@GeorgeCushen&lt;/a&gt; &lt;a href=&#34;https://twitter.com/search?q=%28%23MadeWithWowchemy%20OR%20%23MadeWithAcademic%29&amp;amp;src=typed_query&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#MadeWithWowchemy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;💡 &lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-modules/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Request a &lt;strong&gt;feature&lt;/strong&gt; or report a &lt;strong&gt;bug&lt;/strong&gt; for &lt;em&gt;Wowchemy&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;⬆️ &lt;strong&gt;Updating Wowchemy?&lt;/strong&gt; View the &lt;a href=&#34;https://wowchemy.com/docs/hugo-tutorials/update/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Update Tutorial&lt;/a&gt; and &lt;a href=&#34;https://wowchemy.com/updates/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Release Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;crowd-funded-open-source-software&#34;&gt;Crowd-funded open-source software&lt;/h2&gt;
&lt;p&gt;To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.&lt;/p&gt;
&lt;h3 id=&#34;-click-here-to-become-a-sponsor-and-help-support-wowchemys-future-httpswowchemycomplans&#34;&gt;&lt;a href=&#34;https://wowchemy.com/plans/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;❤️ Click here to become a sponsor and help support Wowchemy&amp;rsquo;s future ❤️&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;As a token of appreciation for sponsoring, you can &lt;strong&gt;unlock &lt;a href=&#34;https://wowchemy.com/plans/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;these&lt;/a&gt; awesome rewards and extra features 🦄✨&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;ecosystem&#34;&gt;Ecosystem&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/wowchemy/hugo-academic-cli&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugo Academic CLI&lt;/a&gt;:&lt;/strong&gt; Automatically import publications from BibTeX&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;inspiration&#34;&gt;Inspiration&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://academic-demo.netlify.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Check out the latest &lt;strong&gt;demo&lt;/strong&gt;&lt;/a&gt; of what you&amp;rsquo;ll get in less than 10 minutes, or &lt;a href=&#34;https://wowchemy.com/user-stories/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;view the &lt;strong&gt;showcase&lt;/strong&gt;&lt;/a&gt; of personal, project, and business sites.&lt;/p&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Page builder&lt;/strong&gt; - Create &lt;em&gt;anything&lt;/em&gt; with &lt;a href=&#34;https://wowchemy.com/docs/page-builder/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;widgets&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://wowchemy.com/docs/content/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;elements&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Edit any type of content&lt;/strong&gt; - Blog posts, publications, talks, slides, projects, and more!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Create content&lt;/strong&gt; in &lt;a href=&#34;https://wowchemy.com/docs/content/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Markdown&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://wowchemy.com/docs/import/jupyter/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Jupyter&lt;/strong&gt;&lt;/a&gt;, or &lt;a href=&#34;https://wowchemy.com/docs/install-locally/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;RStudio&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Plugin System&lt;/strong&gt; - Fully customizable &lt;a href=&#34;https://wowchemy.com/docs/customization/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;color&lt;/strong&gt; and &lt;strong&gt;font themes&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Display Code and Math&lt;/strong&gt; - Code highlighting and &lt;a href=&#34;https://en.wikibooks.org/wiki/LaTeX/Mathematics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LaTeX math&lt;/a&gt; supported&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Integrations&lt;/strong&gt; - &lt;a href=&#34;https://analytics.google.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Analytics&lt;/a&gt;, &lt;a href=&#34;https://disqus.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Disqus commenting&lt;/a&gt;, Maps, Contact Forms, and more!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Beautiful Site&lt;/strong&gt; - Simple and refreshing one page design&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Industry-Leading SEO&lt;/strong&gt; - Help get your website found on search engines and social media&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Media Galleries&lt;/strong&gt; - Display your images and videos with captions in a customizable gallery&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mobile Friendly&lt;/strong&gt; - Look amazing on every screen with a mobile friendly version of your site&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-language&lt;/strong&gt; - 34+ language packs including English, 中文, and Português&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-user&lt;/strong&gt; - Each author gets their own profile page&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Privacy Pack&lt;/strong&gt; - Assists with GDPR&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stand Out&lt;/strong&gt; - Bring your site to life with animation, parallax backgrounds, and scroll effects&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;One-Click Deployment&lt;/strong&gt; - No servers. No databases. Only files.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;p&gt;Wowchemy and its templates come with &lt;strong&gt;automatic day (light) and night (dark) mode&lt;/strong&gt; built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the &lt;a href=&#34;https://academic-demo.netlify.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Demo&lt;/a&gt; to see it in action! Day/night mode can also be disabled by the site admin in &lt;code&gt;params.toml&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/customization&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Choose a stunning &lt;strong&gt;theme&lt;/strong&gt; and &lt;strong&gt;font&lt;/strong&gt;&lt;/a&gt; for your site. Themes are fully customizable.&lt;/p&gt;
&lt;h2 id=&#34;license&#34;&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright 2016-present &lt;a href=&#34;https://georgecushen.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;George Cushen&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Released under the &lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-modules/blob/master/LICENSE.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIT&lt;/a&gt; license.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning Driven Wireless Real-time Human Activity Recognition</title>
      <link>https://example.com/publication/guo-2020-deep/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/guo-2020-deep/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Surfingattack: Interactive hidden attack on voice assistants using ultrasonic guided waves</title>
      <link>https://example.com/publication/yan-2020-surfingattack/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/yan-2020-surfingattack/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://example.com/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://example.com/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-wowchemy&#34;&gt;Create slides in Markdown with Wowchemy&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wowchemy&lt;/a&gt; | &lt;a href=&#34;https://owchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;span class=&#34;fragment &#34; &gt;
   One 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   **Two** 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   Three 
&lt;/span&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/media/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-modules/discussions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Automated labeling for robotic autonomous navigation through multi-sensory semi-supervised learning on big data</title>
      <link>https://example.com/publication/xu-2019-automated/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/xu-2019-automated/</guid>
      <description></description>
    </item>
    
    <item>
      <title>DSIC: Deep learning based self-interference cancellation for in-band full duplex wireless</title>
      <link>https://example.com/publication/guo-2019-dsic/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/guo-2019-dsic/</guid>
      <description></description>
    </item>
    
    <item>
      <title>In-band full duplex wireless communications and networking for iot devices: Progress, challenges and opportunities</title>
      <link>https://example.com/publication/wu-2019-band/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/wu-2019-band/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Real-time human activity recognition based on radar</title>
      <link>https://example.com/publication/guo-2019-real/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/guo-2019-real/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A deep residual convolutional neural network for facial keypoint detection with missing labels</title>
      <link>https://example.com/publication/wu-2018-deep/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/wu-2018-deep/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Distance based user localization and tracking with mechanical ultrasonic beamforming</title>
      <link>https://example.com/publication/zhu-2018-distance/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/zhu-2018-distance/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Indoor human activity recognition based on ambient radar with signal processing and machine learning</title>
      <link>https://example.com/publication/zhu-2018-indoor/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/zhu-2018-indoor/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Indoor multi-sensory self-supervised autonomous mobile robotic navigation</title>
      <link>https://example.com/publication/xu-2018-indoor/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/xu-2018-indoor/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Non-contact non-invasive heart and respiration rates monitoring with MIMO radar sensing</title>
      <link>https://example.com/publication/liu-2018-non/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/liu-2018-non/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Real Time 3D Indoor Human Image Capturing Based on FMCW Radar</title>
      <link>https://example.com/publication/guo-2018-real/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/guo-2018-real/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Realtime software defined self-interference cancellation based on machine learning for in-band full duplex wireless communications</title>
      <link>https://example.com/publication/guo-2018-realtime/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/guo-2018-realtime/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Removing Background with Semantic Segmentation Based on Ensemble Learning</title>
      <link>https://example.com/publication/xu-2018-removing/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/xu-2018-removing/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Avoidance of manual labeling in robotic autonomous navigation through multi-sensory semi-supervised learning</title>
      <link>https://example.com/publication/xu-2017-avoidance/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/xu-2017-avoidance/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Real Time Video Stitching by Exploring Temporal and Spatial Features</title>
      <link>https://example.com/publication/wu-2017-real/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/wu-2017-real/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Ultrasound Enabled Speaker Authentication</title>
      <link>https://example.com/project/example/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/example/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://example.com/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.com/admin/config.yml</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://example.com/publication/example/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/example/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
