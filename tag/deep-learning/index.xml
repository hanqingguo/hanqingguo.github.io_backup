<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning | Hanqing&#39;s Homepage</title>
    <link>https://example.com/tag/deep-learning/</link>
      <atom:link href="https://example.com/tag/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Deep Learning</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 20 Jun 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://example.com/media/icon_huf89e5308245d1285fedf398a415e3606_64611_512x512_fill_lanczos_center_3.png</url>
      <title>Deep Learning</title>
      <link>https://example.com/tag/deep-learning/</link>
    </image>
    
    <item>
      <title>YOLO V3 Implementation with Pytorch &amp; Change Language</title>
      <link>https://example.com/project/yolo/</link>
      <pubDate>Thu, 20 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/yolo/</guid>
      <description>&lt;h2&gt;source code&lt;/h2&gt;
This link behind shows how to implement &lt;a href=&#34;https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/&#34;&gt;YOLOv3 with pytorch instructions&lt;/a&gt;
In my tutorial, I give a solution to convert Label to Chinese Character.
&lt;h2&gt;4 Steps To Convert Label to Chinese&lt;/h2&gt;
&lt;h3&gt;1. Change coconame&lt;/h3&gt;
&lt;p&gt; First of all, we need to translate english label to chinese label
&lt;a href=&#34;https://github.com/hanqingguo/YoloV3Pytorch/blob/master/data/coco.names&#34;&gt;translated labels is availiable here&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;2. Use FreeType to write a script to draw chinese characters&lt;/h3&gt;
&lt;br&gt;
&lt;p&gt; Because Opencv doesn&#39;t support chinese character, we need to use a known font style to draw chinese character, it contains 3 parts:
  &lt;ol&gt;
&lt;li&gt;&lt;code&gt;draw_ft_bitmap(self, img, bitmap, pen, color)&lt;/code&gt;&lt;br&gt;
draw each char &lt;br&gt;
&lt;pre&gt;
    :param bitmap: bitmap
    :param pen:    pen
    :param color:  pen color e.g.(0,0,255) - red
    :return:       image
&lt;/pre&gt;
 &lt;/li&gt;
&lt;li&gt;&lt;code&gt;draw_string(self, img, x_pos, y_pos, text, color)&lt;/code&gt;&lt;br&gt;
  draw string &lt;br&gt;
&lt;pre&gt;
    :param x_pos: text x-postion on img
    :param y_pos: text y-postion on img
    :param text:  text (unicode)
    :param color: text color
    :return:      image
&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;draw_text(self, image, pos, text, text_size, text_color)&lt;/code&gt;&lt;br&gt;
  (draw chinese(or not) text with ttf &lt;br&gt;
&lt;pre&gt;
    :param image:     image(numpy.ndarray) to draw text
    :param pos:       where to draw text
    :param text:      the context, for chinese should be unicode type
    :param text_size: text size
    :param text_color:text color
    :return:          image
&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/p&gt;
&lt;h3&gt;3. Download msyh.ttf&lt;/h3&gt;
&lt;p&gt; .ttf file is dependency to specify font style, it would be loaded in FreeType script, click &lt;a href=&#34;https://github.com/hanqingguo/YoloV3Pytorch/blob/master/msyh.ttf&#34;&gt;here&lt;/a&gt; to Download &lt;/p&gt;
&lt;h3&gt;4. Apply ft2 Chinese Character to yolo&lt;/h3&gt;
&lt;p&gt;In main detect flow (cam_detect.py), instead of using &lt;code&gt;cv2.imshow()&lt;/code&gt;, we use &lt;code&gt;ft.draw_text()&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;
   comment &lt;code&gt;cv2.putText&lt;/code&gt; part, use &lt;code&gt;ft.draw_text&lt;/code&gt; instead &lt;br&gt;
&lt;pre&gt;
    # c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4
    # cv2.rectangle(img, c1, c2,color, -1)
    # cv2.rectangle(img, c1, c2, color, -1)
    # cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 2, [225,255,255], 1);
    ft = ft2.put_chinese_text(&#39;msyh.ttf&#39;)
    ft.draw_text(image=img, pos=(c1[0], c1[1] + t_size[1] - 7), text=label, text_size=15, text_color=[255, 255, 255])
&lt;/pre&gt;&lt;/p&gt;
&lt;h3&gt;Before Translate&lt;/h3&gt;
&lt;video width=&#34;400&#34; controls&gt;
&lt;source src=&#34;eng_caption.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;br/&gt;
&lt;h3&gt;After Translate&lt;/h3&gt;
&lt;video width=&#34;400&#34; controls&gt;
&lt;source src=&#34;cn_caption.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;/br&gt;
&lt;p&gt;You can Download All source code from my github,&lt;a href=&#34;https://github.com/hanqingguo/YoloV3Pytorch&#34;&gt;here&lt;/a&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Walabot Shows 3D Image</title>
      <link>https://example.com/project/radar/</link>
      <pubDate>Sat, 27 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/radar/</guid>
      <description>&lt;style&gt;
.column {
  float: left;
  width: 50%;
  padding: 5px;
}

/* Clear floats after image containers */
.row::after {
  content: &#34;&#34;;
  clear: both;
  display: table;
}
&lt;/style&gt;
&lt;p&gt;&lt;a href=&#34;https://walabot.com/&#34;&gt;Walabot&lt;/a&gt; senses environment by transmitting, receiving and recording signals from MIMO antennas. The frequency range from 3.3-10 GHz.
Today I am going to show you how walabot collect 3D images.&lt;/p&gt;
&lt;h2&gt;Raw signals&lt;/h2&gt;
  &lt;img src=&#34;raw.PNG&#34; width=&#34;50%&#34;/&gt;
  &lt;br/&gt;
  &lt;p&gt;
  Our radar sensing platform emits probing pulse signals x(t) at a pulse repetition frequency (PRF) of 16 Hz, but within each pulse repetition interval (PRI), the receiver antenna
samples the received signal y(t) at a very high frequency of 8 KHz.
  The x-axis of raw signals is response time, while y-axis means amplitude at that time slot. Red line is actually samples what receiver antenna sampling.
  In other words, response time is signal traverse time from transmit antenna to receive antenna, which depends on distance to radar. Higher amplitude at specific time, means there is object at that place.
  &lt;/p&gt;
&lt;h2&gt;2D images&lt;/h2&gt;
&lt;div class=&#34;row&#34;&gt;
  &lt;div class=&#34;column&#34;&gt;
    &lt;img src=&#34;2d.PNG&#34; style=&#34;width:100%&#34;&gt;
  &lt;/div&gt;
  &lt;div class=&#34;column&#34;&gt;
    &lt;img src=&#34;2d-real.png&#34; height=60% style=&#34;width:100%&#34;&gt;
  &lt;/div&gt;
&lt;/div&gt;
  &lt;p&gt;While 2D images only shows &amp;phi; (wide angle) versus &lt;var&gt;R&lt;/var&gt;;. However, any object in real world is 3D, it has height as well, then introduce &amp;theta; (elevation angle) as height.
  To get 3D image, let&#39;s see the axis system in walabot.&lt;/p&gt;
    &lt;img src=&#34;axis.png&#34; width=&#34;50%&#34; height=&#34;50%&#34;/&gt;&lt;br&gt;
    where &lt;br&gt;
    &lt;code&gt;
      X = R*Sin&amp;theta;&lt;br&gt;
      Y = R*Cos&amp;theta;Sin&amp;phi;&lt;br&gt;
      Z = R*Cos&amp;theta;Cos&amp;phi;
    &lt;/code&gt;
&lt;h2&gt;3D images&lt;/h2&gt;
  &lt;p&gt;Instead of using 2D images, we construct 3D images based on those 2D images by stacking them in vertical direction.&lt;/p&gt;
  &lt;img src=&#34;3d-stack.png&#34; width=&#34;85%&#34;/&gt;
  &lt;p&gt;Figure above shows how to stack 2D images, the implement process are
    &lt;ol&gt;
  &lt;li&gt;Concatenate 2D images to 3D matrix &lt;/li&gt;
  &lt;li&gt;Use &lt;code&gt;measure.marching_cubes_classic&lt;/code&gt; to make vertices and faces&lt;/li&gt;
  &lt;li&gt;Change axis ranges from interval index to real unit&lt;br&gt;
    For example: &lt;br&gt;
    &lt;code&gt;
    (0,100) =&gt; (1,200)  R(cm)&lt;br&gt;
    (0,61)  =&gt; (-90,90) &amp;phi;(degree)&lt;br&gt;
    (0,9)   =&gt; (-20,20) &amp;theta;(degree)
    &lt;/code&gt;
  &lt;/li&gt;
  &lt;li&gt;Use &lt;code&gt;Poly3DCollection&lt;/code&gt; to get mesh&lt;/li&gt;
&lt;/ol&gt;
  &lt;/p&gt;
&lt;h2&gt;3D videos&lt;/h2&gt;
  &lt;p&gt;Once get 3D image, we save it to IO buffer, and use PIL to open buffer, then convert it to ndarray, write ndarray as one frame to video by using openCV&lt;/p&gt;
  &lt;video width=&#34;400&#34; controls&gt;
  &lt;source src=&#34;walk.mp4&#34; type=&#34;video/mp4&#34;&gt;
  &lt;/video&gt;
  &lt;p&gt;Video above shows a human walk around walabot radar.&lt;/p&gt;
&lt;h2&gt;CNN extract Features&lt;/h2&gt;
&lt;p&gt;For each frame, we use resnet-18 to extract features, we change last Average pooling to Max pooling because Max pooling extracts the most important features like edges whereas,
  average pooling extracts features so smoothly. For image data, you can see the difference. Although both are used for same reason, I think max pooling is better for extracting the extreme features.
  Average pooling sometimes canâ€™t extract good features because it takes all into count and results an average value which may/may not be important for object detection type tasks.
  Then change output linear layer to extract 10 features.
&lt;/p&gt;
&lt;h2&gt;LSTM training&lt;/h2&gt;
  &lt;p&gt;&lt;img src=&#34;lstm.png&#34; width=&#34;45%&#34; height=&#34;60%&#34;/&gt;&lt;br&gt;
    &lt;br&gt;
    We use 3 frames to recognize one activity. Since 3D radar signal shown above is abstract video. Unlike camera videos which each frame represent a activity, radar video can only be detected by
    continuous frames change to recognize one activity. We collect many 3 frames video for each activity as training data. When testing, given continuous stream radar video, and feed every 3 frames to
    LSTM network, then gives our prediction for each frame.
    Detailed technique will be present in my next paper.
  &lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deepfakes Implementation with Pytorch</title>
      <link>https://example.com/project/deepfake/</link>
      <pubDate>Thu, 27 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/deepfake/</guid>
      <description>&lt;h2&gt;What is Deepfakes&lt;/h2&gt;
Basically, Deepfakes is an unsupervised machine learning algorithm.&lt;br&gt;
It trains &lt;code&gt;one encoder&lt;/code&gt; and &lt;code&gt;two decoders&lt;/code&gt; to process &lt;code&gt;person A&lt;/code&gt; and &lt;code&gt;person B&lt;/code&gt;, the loss can be calculated by difference between ground truth image and decoded image.&lt;br&gt;
For example, it trains an &lt;code&gt;encoder(e)&lt;/code&gt; to extract person A features, and trains a &lt;code&gt;decoder(dA)&lt;/code&gt; to decode A&#39;s feature and produce fake A&#39;s &lt;code&gt;image(fA)&lt;/code&gt;. Through comparing difference between &lt;code&gt;original image&lt;/code&gt; and &lt;code&gt;fA&lt;/code&gt;, to make encoder and decoder work well. &lt;br&gt;
Similarly, the same &lt;code&gt;encoder(e)&lt;/code&gt; also extracts B&#39;s feature,
and trains a &lt;code&gt;decoder(dB)&lt;/code&gt; to decode B&#39;s feature and produce fake B&#39;s &lt;code&gt;image(fB)&lt;/code&gt;.&lt;br&gt;
&lt;p&gt;&lt;font color=&#34;blue&#34;&gt;
Think about it, what if given a image of A, and use &lt;code&gt;(dB)&lt;/code&gt; to decode, what will happens?&lt;/font&gt;&lt;/p&gt;&lt;br&gt;
&lt;p&gt;&lt;font size=&#34;+2&#34;&gt;
Here is the magic:&lt;/font&gt;&lt;/p&gt;
&lt;img src=&#34;face.jpg&#34; width=&#34;85%&#34;/&gt;
&lt;h2&gt;Basic Concept&lt;/h2&gt;
This basic concept of deepfake can also be found here: &lt;a href=&#34;https://www.alanzucconi.com/2018/03/14/understanding-the-technology-behind-deepfakes/&#34;&gt;understanding Deepfakes &lt;/a&gt;
or chinese version &lt;a href=&#34;https://oldpan.me/archives/deepfake-autoencoder-face-swap&#34;&gt; Good Tutorial of Chinese version Deepfakes&lt;/a&gt;
In my tutorial, I give a solution to use pytorch implement deepfakes and change trump&#39;s presentation video with my face!
&lt;h2&gt;5 Steps To Use My code&lt;/h2&gt;
find my code at &lt;a href=&#34;https://github.com/hanqingguo/deepfake-pytorch&#34;&gt;My Code&lt;/a&gt; and git clone to an empty repo.
&lt;h3&gt;Requirement:&lt;/h3&gt;
&lt;pre&gt;
python == 3.6
pytorch &amp;gt= 0.4.0 or 1.0
dlib
&lt;/pre&gt;
&lt;h3&gt;1. Construct Dataset&lt;/h3&gt;
&lt;p&gt; First of all, we need to constrcut our dataset.&lt;br&gt;&lt;/p&gt;
&lt;pre&gt;
$ cd #current directory#
$ mkdir train
$ cd train
&lt;/pre&gt;
&lt;p&gt;Put videos of A and B to train/ , for example, trump.mp4 and me.mp4 where A is trump, B is myself &lt;/p&gt;
&lt;h3&gt;2. Crop frames from videos&lt;/h3&gt;
In train/ directory, make two subdirectory to store all frames of videos
&lt;pre&gt;
$ mkdir personA
$ mkdir personB
&lt;/pre&gt;
Run &lt;pre&gt;python crop_from_video.py&lt;/pre&gt; to save frames to &lt;code&gt;personA&lt;/code&gt; and &lt;code&gt;personB&lt;/code&gt; directory. &lt;br&gt;
&lt;p&gt;Make sure change &lt;code&gt;Video_Path&lt;/code&gt; and &lt;code&gt;save_path&lt;/code&gt; parameter in the python file. Do it twice to crop trump video to &lt;code&gt;personA&lt;/code&gt; directory, crop my video to &lt;code&gt;personB&lt;/code&gt; directory.&lt;/p&gt;
&lt;h3&gt;3. Collect faces from frames&lt;/h3&gt;
In train/ directory, make two subdirectory to store faces of all frames.
&lt;pre&gt;
$ mkdir personA_face
&lt;/pre&gt;
#(To save faces of person A from personA)
&lt;pre&gt;
$ mkdir personB_face
&lt;/pre&gt;
 #(To save faces of person A from personA)&lt;br&gt;
&lt;p&gt;Run &lt;pre&gt;python crop_face.py&lt;/pre&gt; use &lt;code&gt;dlib&lt;/code&gt; to crop faces from frames and save to &lt;code&gt;personA_face&lt;/code&gt; and &lt;code&gt;personB_face&lt;/code&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;## Make sure change Image_Folder and OutFace_Folder parameter in the python file. Do it twice to crop trump face to &lt;code&gt;personA_face&lt;/code&gt; directory, crop myselft face to personB_face directory.
&lt;/p&gt;
&lt;h3&gt;4. Train Model&lt;/h3&gt;
&lt;pre&gt;python train.py&lt;/pre&gt;
&lt;h3&gt;5. Load Model and Output video with my face and trump body&lt;/h3&gt;
&lt;pre&gt;python convert_video.py&lt;/pre&gt;
&lt;h3&gt;Interesting!&lt;/h3&gt;
&lt;video width=&#34;400&#34; controls&gt;
&lt;source src=&#34;me4_out.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;br/&gt;
</description>
    </item>
    
  </channel>
</rss>
